{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be2be71",
   "metadata": {},
   "source": [
    "# Recreating Roelof's PPA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1a6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as pyplot \n",
    "\n",
    "STEP_SIZE = 25   # duration time step in ms\n",
    "N_STEPs = 80     # 2000 ms in total\n",
    "N_CONCEPTs = 5   \n",
    "N_LEMMAs = 5     \n",
    "N_MORPHEMEs = 5  \n",
    "N_PHONEMEs = 10   \n",
    "N_SYLLABLEs = 5  \n",
    "\n",
    "N_lesion_values = 100 # for 100 for weight lesion, 66 (!) for decay lesion\n",
    "\n",
    "N_GROUPs = 4 # Normal, Nonfluent_agrammatic, Semantic_dementia, Logopenic\n",
    "NORMAL = 0\n",
    "NONFLUENT_AGRAMMATIC = 1\n",
    "SEMANTIC_DEMENTIA = 2\n",
    "LOGOPENIC = 3\n",
    "\n",
    "N_TASKs = 3 # Naming, Comprehension, Repetition\n",
    "NAMING = 0\n",
    "COMPREHENSION = 1\n",
    "REPETITION = 2\n",
    "\n",
    "N_ASSESSMENTs = 6\n",
    "ENGLISH = 0\n",
    "DUTCH = 1\n",
    "BRAMBATI_T1 = 2 # baseline \n",
    "BRAMBATI_T2 = 3 # follow up \n",
    "ROHRERMANDELLI_T1 = 4 # baseline \n",
    "ROHRERMANDELLI_T2 = 5 # follow up \n",
    "\n",
    "Y = 1.0     # connection present \n",
    "N = 0.0     # connection absent\n",
    "\n",
    "# labeling network nodes\n",
    "CAT = 0\n",
    "DOG = 1\n",
    "MAT = 2\n",
    "FOG = 3\n",
    "FISH = 4\n",
    "\n",
    "pK = 0 # phonemes\n",
    "pE = 1\n",
    "pT = 2\n",
    "pD = 3\n",
    "pO = 4\n",
    "pG = 5\n",
    "pM = 6\n",
    "pF = 7\n",
    "pI = 8\n",
    "pS = 9\n",
    "\n",
    "Cat = 0\n",
    "Dog = 1\n",
    "Mat = 2\n",
    "Fog = 3\n",
    "Fish = 4\n",
    "\n",
    "# connections conceptual network [N_CONCEPTs][N_CONCEPTs]\n",
    "CC_con =  np.array([\n",
    "    # CAT   DOG  MAT  FOG  FISH  \n",
    "    [   N,    Y,   N,   N,    Y ], # CAT\n",
    "    [   Y,    N,   N,   N,    Y ], # DOG\n",
    "    [   N,    N,   N,   N,    N ], # MAT\n",
    "    [   N,    N,   N,   N,    N ], # FOG\n",
    "    [   Y,    Y,   N,   N,    N ]  # FISH\n",
    "])\n",
    "\n",
    "# connections between concept and lemma nodes [N_CONCEPTs][N_LEMMAs]\n",
    "CL_con = np.array([\n",
    "    [ Y,  N,  N,  N,  N ],\n",
    "    [ N,  Y,  N,  N,  N ],\n",
    "    [ N,  N,  Y,  N,  N ],\n",
    "    [ N,  N,  N,  Y,  N ],\n",
    "    [ N,  N,  N,  N,  Y ]\n",
    "])\n",
    "\n",
    "# connections between lemma nodes and morpheme nodes [N_LEMMAs][N_MORPHEMEs]\n",
    "LM_con = np.array([\n",
    "    [ Y,  N,  N,  N,  N ],\n",
    "    [ N,  Y,  N,  N,  N ],\n",
    "    [ N,  N,  Y,  N,  N ],\n",
    "    [ N,  N,  N,  Y,  N ],\n",
    "    [ N,  N,  N,  N,  Y ]\n",
    "])\n",
    "\n",
    "# connections between morpheme nodes and output phoneme nodes [N_MORPHEMEs][N_PHONEMEs]\n",
    "MP_con = np.array([\n",
    "     #  K  E  T  D  O  G  M  F  I  S  \n",
    "    [   Y, Y, Y, N, N, N, N, N, N, N ], # <cat>\n",
    "    [   N, N, N, Y, Y, Y, N, N, N, N ], # <dog>\n",
    "    [   N, Y, Y, N, N, N, Y, N, N, N ], # <mat>\n",
    "    [   N, N, N, N, Y, Y, N, Y, N, N ], # <fog>\n",
    "    [   N, N, N, N, N, N, N, Y, Y, Y ]  # <fish>\n",
    "])\n",
    "\n",
    "# connections between output phoneme nodes and syllable program nodes [N_PHONEMEs][N_SYLLABLEs]\n",
    "PS_con = np.array([\n",
    "    # Cat Dog  Mat Fog  Fish\n",
    "    [ Y,   N,   N,  N,   N ], # K\n",
    "    [ Y,   N,   Y,  N,   N ], # E\n",
    "    [ Y,   N,   Y,  N,   N ], # T\n",
    "    [ N,   Y,   N,  N,   N ], # D\n",
    "    [ N,   Y,   N,  Y,   N ], # O\n",
    "    [ N,   Y,   N,  Y,   N ], # G\n",
    "    [ N,   N,   Y,  N,   N ], # M\n",
    "    [ N,   N,   N,  Y,   Y ], # F\n",
    "    [ N,   N,   N,  N,   Y ], # I\n",
    "    [ N,   N,   N,  N,   Y ]  # S\n",
    "])\n",
    "\n",
    "# connections between input and output phoneme nodes [N_PHONEMEs][N_PHONEMEs]\n",
    "PP_con = np.array([\n",
    "    # K    E    T   D    O    G   M   F   I  S \n",
    "    [ Y,   N,   N,  N,   N,   N,  N,  N,  N, N  ], # K\n",
    "    [ N,   Y,   N,  N,   N,   N,  N,  N,  N, N  ], # E\n",
    "    [ N,   N,   Y,  N,   N,   N,  N,  N,  N, N  ], # T\n",
    "    [ N,   N,   N,  Y,   N,   N,  N,  N,  N, N  ], # D\n",
    "    [ N,   N,   N,  N,   Y,   N,  N,  N,  N, N  ], # O\n",
    "    [ N,   N,   N,  N,   N,   Y,  N,  N,  N, N  ], # G\n",
    "    [ N,   N,   N,  N,   N,   N,  Y,  N,  N, N  ], # M\n",
    "    [ N,   N,   N,  N,   N,   N,  N,  Y,  N, N  ], # F\n",
    "    [ N,   N,   N,  N,   N,   N,  N,  N,  Y, N  ], # I\n",
    "    [ N,   N,   N,  N,   N,   N,  N,  N,  N, Y  ]  # S\n",
    "])\n",
    "\n",
    "# connections between input phoneme nodes and input morpheme nodes [N_PHONEMEs][N_MORPHEMEs]\n",
    "PiM_con = np.array([\n",
    "    # Cat Dog  Mat Fog  Fish \n",
    "    [ Y,   N,   N,  N,   N ], # K\n",
    "    [ Y,   N,   Y,  N,   N ], # E\n",
    "    [ Y,   N,   Y,  N,   N ], # T\n",
    "    [ N,   Y,   N,  N,   N ], # D\n",
    "    [ N,   Y,   N,  Y,   N ], # O\n",
    "    [ N,   Y,   N,  Y,   N ], # G\n",
    "    [ N,   N,   Y,  N,   N ], # M\n",
    "    [ N,   N,   N,  Y,   Y ], # F\n",
    "    [ N,   N,   N,  N,   Y ], # I\n",
    "    [ N,   N,   N,  N,   Y ]  # S\n",
    "])\n",
    "\n",
    "# connections between input morpheme and output morpheme nodes [N_MORPHEMEs][N_MORPHEMEs]\n",
    "iMM_con = np.array([\n",
    "    [ Y,  N,  N,  N,  N ],\n",
    "    [ N,  Y,  N,  N,  N ],\n",
    "    [ N,  N,  Y,  N,  N ],\n",
    "    [ N,  N,  N,  Y,  N ],\n",
    "    [ N,  N,  N,  N,  Y ]\n",
    "])\n",
    "\n",
    "# connections between input morpheme and lemma nodes [N_MORPHEMEs][N_LEMMAs]\n",
    "iML_con = np.array([\n",
    "    [ Y,  N,  N,  N,  N ],\n",
    "    [ N,  Y,  N,  N,  N ],\n",
    "    [ N,  N,  Y,  N,  N ],\n",
    "    [ N,  N,  N,  Y,  N ],\n",
    "    [ N,  N,  N,  N,  Y ]\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf84543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English data on PPA for single word tasks: Savage et al. (2013) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_ENGLISH = np.array([\n",
    "    # Naming  Comprehension Repetition\n",
    "    [ 88.7,      97.0,      99.7 ], # Control\n",
    "    [ 78.3,      94.3,      79.7 ], # nfvPPA\n",
    "    [ 22.7,      63.3,      95.3 ], # svPPA\n",
    "    [ 41.3,      84.7,      84.7 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# Dutch data on PPA for single word tasks: Janssen et al. (2021) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_DUTCH = np.array([\n",
    "    # Naming  Comprehension Repetition */\n",
    "    [ 90.3,      96.3,      96.7 ], # Control\n",
    "    [ 77.3,      97.7,      89.3 ], # nfvPPA\n",
    "    [ 29.0,      78.0,      96.3 ], # svPPA\n",
    "    [ 66.3,      93.7,      91.3 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# Brambati T1 data on PPA for single word tasks: Brambati et al. (2015) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_BRAMBATI_T1 = np.array([\n",
    "    # Naming  Comprehension Repetition \n",
    "    [ 90.3,      96.3,      96.7 ], # Control (dummy, from Savage)\n",
    "    [ 85.3,      99.7,      83.7 ], # nfvPPA\n",
    "    [ 26.7,      88.0,      90.6 ], # svPPA\n",
    "    [ 69.3,      95.0,      69.0 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# Brambati T2 data on PPA for single word tasks: Brambati et al. (2015) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_BRAMBATI_T2 = np.array([\n",
    "    # Naming  Comprehension Repetition\n",
    "    [ 90.3,      96.3,      96.7 ], # Control (dummy, from Savage)\n",
    "    [ 83.3,      94.8,      68.0 ], # nfvPPA\n",
    "    [ 19.3,      66.7,      82.3 ], # svPPA\n",
    "    [ 52.7,      95.0,      58.8 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# Rohrer et al. (2013), logopenic patients (N=21), T1 baseline and T2 one year later\n",
    "# Mandelli et al. (2016), nonfluent/agrammatic patients (N=34), T1 baseline and T2 one year later\n",
    "\n",
    "# RohrerMandelli T1 data on PPA: Rohrer et al. (2013), Mandelli et al. (2016) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_ROHRERMANDELLI_T1 = np.array([\n",
    "    # Naming  Comprehension Repetition \n",
    "    [ 90.3,      96.3,      96.7 ], # Control (dummy, from Savage)\n",
    "    [ 76.7,      99.0,      81.5 ], # nfvPPA\n",
    "    [ 26.7,      88.0,      90.6 ], # svPPA (dummy, from Brambati)\n",
    "    [ 61.0,      94.0,      94.0 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# RohrerMandelli T2 data on PPA: Rohrer et al. (2013), Mandelli et al. (2016) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_ROHRERMANDELLI_T2 = np.array([\n",
    "    # Naming  Comprehension Repetition \n",
    "    [ 90.3,      96.3,      96.7 ], # Control (dummy, from Savage)\n",
    "    [ 66.0,      90.0,      65.5 ], # nfvPPA\n",
    "    [ 26.7,      88.0,      90.6 ], # svPPA (dummy, from Brambati)\n",
    "    [ 43.0,      85.0,      77.0 ]  # lvPPA\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532b5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter values\n",
    "CYCLE_TIME = 25                 # ms per link \n",
    "SEM_rate = 0.0101 * STEP_SIZE   # prop per step_size ms \n",
    "LEM_rate = 0.0074 * STEP_SIZE   # prop per step_size ms \n",
    "LEX_rate = 0.0120 * STEP_SIZE   # prop per step_size ms \n",
    "DECAY_rate = 0.0240 * STEP_SIZE # prop per step_size ms \n",
    "EXTIN = 0.1965 * STEP_SIZE      # act_units per step_size ms \n",
    "LEMLEXFRAC = 0.3 \n",
    "# fraction of LEX_rate spread between lemmas and output morphemes \n",
    "# implementing weak cascading of activation, see Roelofs (2008, JEP:LMC) \n",
    "\n",
    "FR = 0.10  # fraction of connection weight for input phoneme to input morpheme, cf. Roelofs (1997, Cognition)\n",
    "SEGMENT_DURATION = 125  # ms\n",
    "PICTURE_DURATION = 125  # ms\n",
    "\n",
    "# set here to simulate weight or decay lesion and what to print \n",
    "WEIGHT_LESION = 0 #1\n",
    "DECAY_LESION = 1 #0\n",
    "\n",
    "SHOW_RESULTS_ALL_VALUES = 0 # set here whether to print all values \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0f4db",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7e408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        CC_con = CC_con,\n",
    "        CL_con = CL_con,\n",
    "        LM_con = LM_con,\n",
    "        MP_con = MP_con,\n",
    "        PS_con = PS_con,\n",
    "        PP_con = PP_con,\n",
    "        PiM_con = PiM_con,\n",
    "        iMM_con = iMM_con,\n",
    "        iML_con = iML_con,\n",
    "        \n",
    "        REAL_DATA = np.zeros(shape=(N_GROUPs, N_TASKs)),\n",
    "        SIM_DATA = np.zeros(shape=(N_GROUPs, N_TASKs)),\n",
    "        GOODNESS_OF_FIT = np.zeros(N_lesion_values),\n",
    "\n",
    "        WEIGHT_value = np.zeros(N_lesion_values),\n",
    "        DECAY_value = np.zeros(N_lesion_values),\n",
    "\n",
    "        # concept and lemma\n",
    "        C_node_act = np.zeros(N_CONCEPTs),\n",
    "        L_node_act = np.zeros(N_LEMMAs),\n",
    "        # output form \n",
    "        M_node_act = np.zeros(N_MORPHEMEs),\n",
    "        oP_node_act = np.zeros(N_PHONEMEs),\n",
    "        S_node_act = np.zeros(N_SYLLABLEs),\n",
    "        # input form \n",
    "        iM_node_act = np.zeros(N_MORPHEMEs),\n",
    "        iP_node_act = np.zeros(N_PHONEMEs),\n",
    "\n",
    "        # input buffer for nodes \n",
    "        input_C = np.zeros(N_CONCEPTs),\n",
    "        input_L = np.zeros(N_LEMMAs),\n",
    "        input_M = np.zeros(N_MORPHEMEs),\n",
    "        input_iM = np.zeros(N_MORPHEMEs),\n",
    "        input_iP = np.zeros(N_PHONEMEs),\n",
    "        input_oP = np.zeros(N_PHONEMEs),\n",
    "        input_S = np.zeros(N_SYLLABLEs),\n",
    "        \n",
    "        # Aphasia parameters\n",
    "\n",
    "        # weight lesion \n",
    "        CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC = 0.0, # connections to/from output phonemes \n",
    "        CONNECTION_DECREASE_SEMANTIC_DEMENTIA = 0.0, # connections to/within/from conceptual network\n",
    "        CONNECTION_DECREASE_LOGOPENIC = 0.0, # connections to/from lexical output forms, and b/w input/output phonemes\n",
    "\n",
    "        # decay lesion\n",
    "        DECAY_INCREASE_NONFLUENT_AGRAMMATIC = 0.0, # output phonemes \n",
    "        DECAY_INCREASE_SEMANTIC_DEMENTIA = 0.0, # concepts\n",
    "        DECAY_INCREASE_LOGOPENIC = 0.0, # lexical output forms\n",
    "\n",
    "        ACT_C = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs)),\n",
    "        ACT_S = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs)),\n",
    "\n",
    "        # Activation of target concept, cat\n",
    "        ACT_CT = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs)),\n",
    "        # Activation of conceptual relative, dog\n",
    "        ACT_CR= np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs)),\n",
    "\n",
    "        # Activation of target lemma, cat\n",
    "        ACT_LT = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs)),\n",
    "        # Activation of lemma relative, i.e., semantically related, dog\n",
    "        ACT_LR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs)),\n",
    "\n",
    "        # Activation of target syllable, cat\n",
    "        ACT_ST = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs)),\n",
    "        # Activation of syllabic relative, mat\n",
    "        ACT_SR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs)),\n",
    "\n",
    "        TOTAL_ACT_C = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        MEAN_ACT_C = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        TOTAL_ACT_S = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        MEAN_ACT_S = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "\n",
    "        # T = target, R = relative\n",
    "        TOTAL_ACT_CT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        MEAN_ACT_CT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        TOTAL_ACT_CR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        MEAN_ACT_CR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "\n",
    "        TOTAL_ACT_LT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        MEAN_ACT_LT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        TOTAL_ACT_LR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        MEAN_ACT_LR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "\n",
    "        TOTAL_ACT_ST = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        MEAN_ACT_ST = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        TOTAL_ACT_SR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "        MEAN_ACT_SR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs)),\n",
    "    ):\n",
    "        \n",
    "        self.CC_con = CC_con\n",
    "        self.CL_con = CL_con\n",
    "        self.LM_con = LM_con\n",
    "        self.MP_con = MP_con\n",
    "        self.PS_con = PS_con\n",
    "        self.PP_con = PP_con\n",
    "        self.PiM_con = PiM_con\n",
    "        self.iMM_con = iMM_con\n",
    "        self.iML_con = iML_con\n",
    "        \n",
    "        self.REAL_DATA = REAL_DATA\n",
    "        self.SIM_DATA = SIM_DATA\n",
    "        self.GOODNESS_OF_FIT = GOODNESS_OF_FIT\n",
    "\n",
    "        self.WEIGHT_value = WEIGHT_value\n",
    "        self.DECAY_value = DECAY_value\n",
    "\n",
    "        self.C_node_act = C_node_act\n",
    "        self.L_node_act = L_node_act\n",
    "        self.M_node_act = M_node_act\n",
    "        self.oP_node_act = oP_node_act\n",
    "        self.S_node_act = S_node_act\n",
    "        self.iM_node_act = iM_node_act\n",
    "        self.iP_node_act = iP_node_act\n",
    "\n",
    "        self.input_C = input_C\n",
    "        self.input_L = input_L\n",
    "        self.input_M = input_M\n",
    "        self.input_iM = input_iM\n",
    "        self.input_iP = input_iP\n",
    "        self.input_oP = input_oP\n",
    "        self.input_S = input_S\n",
    "        \n",
    "        self.CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC = CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC\n",
    "        self.CONNECTION_DECREASE_SEMANTIC_DEMENTIA = CONNECTION_DECREASE_SEMANTIC_DEMENTIA\n",
    "        self.CONNECTION_DECREASE_LOGOPENIC = CONNECTION_DECREASE_LOGOPENIC\n",
    "\n",
    "        self.DECAY_INCREASE_NONFLUENT_AGRAMMATIC = DECAY_INCREASE_NONFLUENT_AGRAMMATIC\n",
    "        self.DECAY_INCREASE_SEMANTIC_DEMENTIA = DECAY_INCREASE_SEMANTIC_DEMENTIA\n",
    "        self.DECAY_INCREASE_LOGOPENIC = DECAY_INCREASE_LOGOPENIC\n",
    "\n",
    "        self.ACT_C = ACT_C\n",
    "        self.ACT_S = ACT_S\n",
    "        self.ACT_CT = ACT_CT\n",
    "        self.ACT_CR = ACT_CR\n",
    "        self.ACT_LT = ACT_LT\n",
    "        self.ACT_LR = ACT_LR\n",
    "        self.ACT_ST = ACT_ST\n",
    "        self.ACT_SR = ACT_SR\n",
    "\n",
    "        self.TOTAL_ACT_C = TOTAL_ACT_C\n",
    "        self.MEAN_ACT_C = MEAN_ACT_C\n",
    "        self.TOTAL_ACT_S = TOTAL_ACT_S\n",
    "        self.MEAN_ACT_S = MEAN_ACT_S\n",
    "\n",
    "        self.TOTAL_ACT_CT = TOTAL_ACT_CT\n",
    "        self.MEAN_ACT_CT = MEAN_ACT_CT\n",
    "        self.TOTAL_ACT_CR = TOTAL_ACT_CR\n",
    "        self.MEAN_ACT_CR = MEAN_ACT_CR\n",
    "\n",
    "        self.TOTAL_ACT_LT = TOTAL_ACT_LT\n",
    "        self.MEAN_ACT_LT = MEAN_ACT_LT\n",
    "        self.TOTAL_ACT_LR = TOTAL_ACT_LR\n",
    "        self.MEAN_ACT_LR = MEAN_ACT_LR\n",
    "\n",
    "        self.TOTAL_ACT_ST = TOTAL_ACT_ST\n",
    "        self.MEAN_ACT_ST = MEAN_ACT_ST\n",
    "        self.TOTAL_ACT_SR = TOTAL_ACT_SR\n",
    "        self.MEAN_ACT_SR = MEAN_ACT_SR\n",
    "        \n",
    "        \n",
    "    def test(self):\n",
    "        print('hi')\n",
    "        \n",
    "    \n",
    "    def set_real_data_matrix(self, assessment):\n",
    "        if assessment == ENGLISH:\n",
    "            self.REAL_DATA = REAL_DATA_ENGLISH\n",
    "        elif assessment == DUTCH:\n",
    "            self.REAL_DATA = REAL_DATA_DUTCH\n",
    "        elif assessment == BRAMBATI_T1:\n",
    "            self.REAL_DATA = REAL_DATA_BRAMBATI_T1\n",
    "        elif assessment == BRAMBATI_T2:\n",
    "            self.REAL_DATA = REAL_DATA_BRAMBATI_T2\n",
    "        elif assessment == ROHRERMANDELLI_T1:\n",
    "            self.REAL_DATA = REAL_DATA_ROHRERMANDELLI_T1\n",
    "        elif (assessment == ROHRERMANDELLI_T2):\n",
    "            self.REAL_DATA = REAL_DATA_ROHRERMANDELLI_T2\n",
    "                    \n",
    "                    \n",
    "    def set_spreading_rates(self):\n",
    "        self.ACT_C = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "        self.ACT_S = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "        self.ACT_CT = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "        self.ACT_CR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "        self.ACT_LT = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "        self.ACT_LR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "        self.ACT_ST = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "        self.ACT_SR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "\n",
    "        self.CC_con *= SEM_rate\n",
    "        self.CL_con *= LEM_rate\n",
    "        self.LM_con *= LEX_rate\n",
    "        self.MP_con *= LEX_rate\n",
    "        self.PS_con *= LEX_rate\n",
    "\n",
    "        # connections for input phonemes to output phonemes, input morphemes, input morphemes to lemmas\n",
    "\n",
    "        self.PP_con *= LEX_rate\n",
    "        self.PiM_con *= (FR * LEX_rate)\n",
    "        self.iMM_con *= LEX_rate\n",
    "        self.iML_con *= LEX_rate\n",
    "        \n",
    "        \n",
    "    def reset_network(self):\n",
    "        self.C_node_act = np.zeros(N_CONCEPTs)\n",
    "        self.L_node_act = np.zeros(N_LEMMAs)\n",
    "        self.M_node_act = np.zeros(N_MORPHEMEs)\n",
    "        self.iM_node_act = np.zeros(N_MORPHEMEs)\n",
    "        self.iP_node_act = np.zeros(N_PHONEMEs)\n",
    "        self.oP_node_act = np.zeros(N_PHONEMEs)\n",
    "        self.S_node_act = np.zeros(N_SYLLABLEs)\n",
    "        \n",
    "        \n",
    "    def set_aphasic_parameters(self, group, lesion_value):\n",
    "        WEIGHT_FACTOR, DECAY_FACTOR = 0.0, 0.0\n",
    "\n",
    "        if WEIGHT_LESION:\n",
    "            WEIGHT_FACTOR = self.WEIGHT_value[lesion_value]  \n",
    "        else:\n",
    "            WEIGHT_FACTOR = 1.0\n",
    "\n",
    "        if DECAY_LESION:\n",
    "            DECAY_FACTOR = self.DECAY_value[lesion_value]   \n",
    "        else:\n",
    "            DECAY_FACTOR = 1.0\n",
    "\n",
    "\n",
    "        # setting of weight parameters\n",
    "\n",
    "        if group == NONFLUENT_AGRAMMATIC:\n",
    "            self.CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC = WEIGHT_FACTOR\n",
    "        else:\n",
    "            self.CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC = 1.0 # normal \n",
    "\n",
    "        if group == SEMANTIC_DEMENTIA:\n",
    "            self.CONNECTION_DECREASE_SEMANTIC_DEMENTIA = WEIGHT_FACTOR\n",
    "        else:\n",
    "            self.CONNECTION_DECREASE_SEMANTIC_DEMENTIA = 1.0 # normal\n",
    "\n",
    "        if group == LOGOPENIC:\n",
    "            self.CONNECTION_DECREASE_LOGOPENIC = WEIGHT_FACTOR\n",
    "        else:\n",
    "            self.CONNECTION_DECREASE_LOGOPENIC = 1.0 # normal\n",
    "\n",
    "        # setting of decay parameters\n",
    "\n",
    "        if group == NONFLUENT_AGRAMMATIC:\n",
    "            self.DECAY_INCREASE_NONFLUENT_AGRAMMATIC = DECAY_FACTOR\n",
    "        else:\n",
    "            self.DECAY_INCREASE_NONFLUENT_AGRAMMATIC = 1.0 # normal\n",
    "\n",
    "        if group == SEMANTIC_DEMENTIA:\n",
    "            self.DECAY_INCREASE_SEMANTIC_DEMENTIA = DECAY_FACTOR\n",
    "        else:\n",
    "            self.DECAY_INCREASE_SEMANTIC_DEMENTIA = 1.0 # normal\n",
    "\n",
    "        if group == LOGOPENIC:\n",
    "            self.DECAY_INCREASE_LOGOPENIC = DECAY_FACTOR\n",
    "        else:\n",
    "            self.DECAY_INCREASE_LOGOPENIC = 1.0 # normal\n",
    "            \n",
    "        \n",
    "    # NETWORK UPDATING ROUTINES \n",
    "\n",
    "    def update_network(self, task, T):\n",
    "        self.set_input_to_zero()\n",
    "        self.get_external_input(task, T)\n",
    "        self.get_internal_input()\n",
    "        self.update_activation_of_nodes()\n",
    "        \n",
    "        \n",
    "    def set_input_to_zero(self):\n",
    "        self.input_C = np.zeros(N_CONCEPTs)\n",
    "        self.input_L = np.zeros(N_LEMMAs)\n",
    "        self.input_M = np.zeros(N_MORPHEMEs)\n",
    "        self.input_iM = np.zeros(N_MORPHEMEs)\n",
    "        self.input_iP = np.zeros(N_PHONEMEs)\n",
    "        self.input_oP = np.zeros(N_PHONEMEs)\n",
    "        self.input_S = np.zeros(N_SYLLABLEs)\n",
    "        \n",
    "        \n",
    "    def get_external_input(self, task, T):\n",
    "        if task == NAMING:\n",
    "            # picture input\n",
    "            if (T >= 0 and T < PICTURE_DURATION):\n",
    "                self.input_C[CAT] += self.CONNECTION_DECREASE_SEMANTIC_DEMENTIA * EXTIN\n",
    "\n",
    "            # enhancement\n",
    "            if ((T >= (0 + 1 * CYCLE_TIME)) and  (T < (1 * CYCLE_TIME + PICTURE_DURATION))):\n",
    "                self.input_C[CAT] += EXTIN\n",
    "\n",
    "        if (task == COMPREHENSION or task == REPETITION): \n",
    "\n",
    "            # spoken word input */\n",
    "            if ((0 <= T) and (T < SEGMENT_DURATION)):\n",
    "                self.input_iP[pK] += EXTIN\n",
    "\n",
    "            if ((SEGMENT_DURATION <= T) and (T < (2 * SEGMENT_DURATION))):\n",
    "                self.input_iP[pE] += EXTIN\n",
    "\n",
    "\n",
    "            if ((2 * SEGMENT_DURATION <= T) and (T < (3 * SEGMENT_DURATION))): \n",
    "                self.input_iP[pT] += EXTIN\n",
    "                \n",
    "                \n",
    "    def get_internal_input(self):\n",
    "        # input activation for concept nodes\n",
    "\n",
    "        for i in range(N_CONCEPTs):\n",
    "            for j in range(N_CONCEPTs):\n",
    "                self.input_C[i] += (self.C_node_act[j] * (self.CC_con[j][i] * self.CONNECTION_DECREASE_SEMANTIC_DEMENTIA))\n",
    "\n",
    "        for i in range(N_CONCEPTs):\n",
    "            for j in range(N_LEMMAs):\n",
    "                self.input_C[i] += (self.L_node_act[j] * self.CL_con[j][i] * self.CONNECTION_DECREASE_SEMANTIC_DEMENTIA)\n",
    "\n",
    "        # input activation for lemma nodes\n",
    "        for i in range(N_LEMMAs):\n",
    "            for j in range(N_CONCEPTs):\n",
    "                self.input_L[i] += (self.C_node_act[j] * self.CL_con[j][i] *  self.CONNECTION_DECREASE_SEMANTIC_DEMENTIA)\n",
    "\n",
    "        for i in range(N_LEMMAs):\n",
    "            for j in range(N_MORPHEMEs):                 \n",
    "                self.input_L[i] += (self.iM_node_act[j] * self.iML_con[j][i])\n",
    "\n",
    "        # input activation for output morpheme nodes\n",
    "        for i in range(N_MORPHEMEs):\n",
    "            for j in range(N_LEMMAs):\n",
    "                self.input_M[i] += (self.L_node_act[j] * LEMLEXFRAC * self.LM_con[j][i] \n",
    "                                    * self.CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "        for i in range(N_MORPHEMEs):\n",
    "            for j in range(N_MORPHEMEs):\n",
    "                self.input_M[i] += (self.iM_node_act[j] * self.iMM_con[j][i] * self.CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "        # input activation for output phoneme nodes\n",
    "        for i in range(N_PHONEMEs):\n",
    "            for j in range(N_MORPHEMEs):\n",
    "                self.input_oP[i] += (self.M_node_act[j] * self.MP_con[j][i] \n",
    "                                     * self.CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC * self.CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "        for i in range(N_PHONEMEs):\n",
    "            for j in range(N_PHONEMEs):\n",
    "                self.input_oP[i] += (self.iP_node_act[j] * self.PP_con[j][i] \n",
    "                                     * self.CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC * self.CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "        # input activation for syllable program nodes\n",
    "        for i in range(N_SYLLABLEs):\n",
    "            for j in range(N_PHONEMEs):\n",
    "                self.input_S[i] += (self.oP_node_act[j] * self.PS_con[j][i] \n",
    "                                    * self.CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC)\n",
    "\n",
    "        # input activation for input phoneme nodes\n",
    "        for i in range(N_PHONEMEs):\n",
    "            for j in range(N_PHONEMEs):\n",
    "                self.input_iP[i] += (self.oP_node_act[j] * self.PP_con[j][i] \n",
    "                                     * self.CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC * self.CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "        # input activation for input morpheme nodes\n",
    "        for i in range(N_MORPHEMEs):\n",
    "            for j in range(N_PHONEMEs):\n",
    "                self.input_iM[i] += (self.iP_node_act[j] * self.PiM_con[j][i])\n",
    "                \n",
    "                \n",
    "    def update_activation_of_nodes(self):\n",
    "        self.C_node_act = np.add(self.C_node_act * (1.0 - (DECAY_rate * self.DECAY_INCREASE_SEMANTIC_DEMENTIA)), self.input_C)\n",
    "        self.L_node_act = np.add(self.L_node_act * (1.0 - DECAY_rate), self.input_L)\n",
    "        self.M_node_act = np.add(self.M_node_act * (1.0 - (DECAY_rate * self.DECAY_INCREASE_LOGOPENIC)), self.input_M)\n",
    "        self.oP_node_act = np.add(self.oP_node_act * (1.0 - (DECAY_rate * self.DECAY_INCREASE_NONFLUENT_AGRAMMATIC)), self.input_oP)\n",
    "        self.iP_node_act = np.add(self.iP_node_act * (1.0 - DECAY_rate), self.input_iP)\n",
    "        self.iM_node_act = np.add(self.iM_node_act * (1.0 - DECAY_rate), self.input_iM)\n",
    "        self.S_node_act = np.add(self.S_node_act * (1.0 - DECAY_rate), self.input_S)\n",
    "        \n",
    "    \"\"\"\n",
    "    FIRST MAP OUT CONNECTIONS (DRAW DIAGRAM)\n",
    "    DEBUG ACCURACIES, AND UNDERSTAND MODEL MORE AND DRAW OUT CONNECTIONS\n",
    "    MOVE TO NENGO, MIGHT BE EASIER TO DO THIS AFTER SIMPLIFYING\n",
    "    WANT TO SET UP TO PLOT ACTIVATIONS AT EACH STEP, KNOW INPUT AND PLOT DYNAMICS\n",
    "    REPLICATE ONE SET OF NODES GIVEN SAME INPUTS, AND MAP OUTPUTS\n",
    "    DO ONE POPULATIONS AT A TIME, DON'T REPLICATE EVERYTHING THEM DEBUG\n",
    "    FIRST POPULATION SHOULD HAVE INPUT FROM OUTSIDE WORLD\n",
    "    \"\"\"     \n",
    "            \n",
    "    \n",
    "    def determine_activation_critical_nodes(self, lesion_value, step, group, task):\n",
    "        self.ACT_C[lesion_value][step][group][task] = self.C_node_act[CAT]\n",
    "        self.ACT_S[lesion_value][step][group][task] = self.S_node_act[CAT]\n",
    "        self.ACT_CT[lesion_value][step][group][task] = self.C_node_act[CAT]\n",
    "        self.ACT_CR[lesion_value][step][group][task] = self.C_node_act[DOG]\n",
    "        self.ACT_LT[lesion_value][step][group][task] = self.L_node_act[CAT]\n",
    "        self.ACT_LR[lesion_value][step][group][task] = self.L_node_act[DOG]\n",
    "        self.ACT_ST[lesion_value][step][group][task] = self.S_node_act[CAT]\n",
    "        self.ACT_SR[lesion_value][step][group][task] = self.S_node_act[MAT]\n",
    "        \n",
    "        \n",
    "    def compute_activation_results(self):\n",
    "        self.TOTAL_ACT_C = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.TOTAL_ACT_S = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.MEAN_ACT_C = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.MEAN_ACT_S = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "\n",
    "        self.TOTAL_ACT_CT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.TOTAL_ACT_CR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.MEAN_ACT_CT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.MEAN_ACT_CR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "\n",
    "        self.TOTAL_ACT_LT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.TOTAL_ACT_LR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.MEAN_ACT_LT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.MEAN_ACT_LR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "\n",
    "        self.TOTAL_ACT_ST = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.TOTAL_ACT_SR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.MEAN_ACT_ST = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "        self.MEAN_ACT_SR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "\n",
    "        for lesion_value in range(N_lesion_values):\n",
    "            for group in range(N_GROUPs):\n",
    "                for task in range(N_TASKs): \n",
    "                    for i in range(N_STEPs): \n",
    "                        self.TOTAL_ACT_C[lesion_value][group][task] += self.ACT_C[lesion_value][i][group][task]\n",
    "                        self.TOTAL_ACT_S[lesion_value][group][task] += self.ACT_S[lesion_value][i][group][task]\n",
    "                        self.TOTAL_ACT_CT[lesion_value][group][task] += self.ACT_CT[lesion_value][i][group][task]\n",
    "                        self.TOTAL_ACT_CR[lesion_value][group][task] += self.ACT_CR[lesion_value][i][group][task]\n",
    "                        self.TOTAL_ACT_LT[lesion_value][group][task] += self.ACT_LT[lesion_value][i][group][task]\n",
    "                        self.TOTAL_ACT_LR[lesion_value][group][task] += self.ACT_LR[lesion_value][i][group][task]\n",
    "                        self.TOTAL_ACT_ST[lesion_value][group][task] += self.ACT_ST[lesion_value][i][group][task]\n",
    "                        self.TOTAL_ACT_SR[lesion_value][group][task] += self.ACT_SR[lesion_value][i][group][task]\n",
    "\n",
    "                    self.MEAN_ACT_C[lesion_value][group][task] = (self.TOTAL_ACT_C[lesion_value][group][task] / N_STEPs)\n",
    "                    self.MEAN_ACT_S[lesion_value][group][task] = (self.TOTAL_ACT_S[lesion_value][group][task] / N_STEPs)\n",
    "                    self.MEAN_ACT_CT[lesion_value][group][task] = (self.TOTAL_ACT_CT[lesion_value][group][task] / N_STEPs)\n",
    "                    self.MEAN_ACT_CR[lesion_value][group][task] = (self.TOTAL_ACT_CR[lesion_value][group][task] / N_STEPs)\n",
    "                    self.MEAN_ACT_LT[lesion_value][group][task] = (self.TOTAL_ACT_LT[lesion_value][group][task] / N_STEPs)\n",
    "                    self.MEAN_ACT_LR[lesion_value][group][task] = (self.TOTAL_ACT_LR[lesion_value][group][task] / N_STEPs)\n",
    "                    self.MEAN_ACT_ST[lesion_value][group][task] = (self.TOTAL_ACT_ST[lesion_value][group][task] / N_STEPs)\n",
    "                    self.MEAN_ACT_SR[lesion_value][group][task] = (self.TOTAL_ACT_SR[lesion_value][group][task] / N_STEPs)\n",
    "                    \n",
    "                    \n",
    "    # FITS AND PRINTING\n",
    "\n",
    "    def print_heading(self):\n",
    "        print(\"\\n\")\n",
    "        print(\"WEAVER++/ARC model simulation of primary progressive aphasia (c) Ardi Roelofs\\n\")\n",
    "        print(\"Simulation of group studies \\n\")\n",
    "        \n",
    "        \n",
    "    def print_parameters(self):\n",
    "        print(\"Parameter values:\\n\")\n",
    "        print(f\"cycle time : {CYCLE_TIME : 6d} [ms]\\n\")\n",
    "        print(f\"sem_rate   : {SEM_rate/STEP_SIZE : .4f} [prop/ms]\\n\")\n",
    "        print(f\"lem_rate   : {LEM_rate/STEP_SIZE : .4f} [prop/ms]\\n\")\n",
    "        print(f\"exin       : {EXTIN/STEP_SIZE : .4f} [act_units/ms]\\n\")\n",
    "        print(f\"d          : {DECAY_rate/STEP_SIZE : .4f} [prop/ms]\\n\")\n",
    "        \n",
    "        \n",
    "    def compute_fits_and_print_results_on_screen(self, assessment):\n",
    "        LV = 0.0 # lesion value\n",
    "\n",
    "        self.SIM_DATA = np.zeros(shape=(N_GROUPs, N_TASKs))\n",
    "        self.GOODNESS_OF_FIT = np.zeros(N_lesion_values)\n",
    "\n",
    "        if assessment == ENGLISH:\n",
    "            print(\"\\nAssessment is Savage et al. (2013), English\\n\")\n",
    "        if assessment == DUTCH:\n",
    "            print(\"\\nAssessment is Janssen et al. (2022), Dutch\\n\")\n",
    "        if assessment == BRAMBATI_T1:\n",
    "            print(\"\\nAssessment is Brambati et al. (2015), baseline T1\\n\")\n",
    "        if assessment == BRAMBATI_T2:\n",
    "            print(\"\\nAssessment is Brambati et al. (2015), follow up T2\\n\")\n",
    "        if assessment == ROHRERMANDELLI_T1:\n",
    "            print(\"\\nAssessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\\n\")\n",
    "        if assessment == ROHRERMANDELLI_T2:\n",
    "            print(\"\\nAssessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\\n\")\n",
    "\n",
    "\n",
    "        for group in range(N_GROUPs): \n",
    "            print(\" \\n\")\n",
    "\n",
    "            if group == NORMAL:\n",
    "                print(\"NORMAL \\n\")\n",
    "            elif group == NONFLUENT_AGRAMMATIC:\n",
    "                print(\"NONFLUENT/AGRAMMATIC \\n\")\n",
    "            elif group == SEMANTIC_DEMENTIA:\n",
    "                print(\"SEMANTIC DEMENTIA  \\n\")\n",
    "            elif group == LOGOPENIC:\n",
    "                print(\"LOGOPENIC  \\n\")\n",
    "\n",
    "            if assessment == ENGLISH:\n",
    "                print(\"\\nSavage et al. (2013), English\\n\")\n",
    "            if assessment == DUTCH:\n",
    "                print(\"\\nAssessment is Janssen et al. (2022), Dutch\\n\")\n",
    "            if assessment == BRAMBATI_T1:\n",
    "                print(\"\\nAssessment is Brambati et al. (2015), baseline T1\\n\")\n",
    "            if assessment == BRAMBATI_T2:\n",
    "                print(\"\\nAssessment is Brambati et al. (2015), follow up T2\\n\")\n",
    "            if assessment == ROHRERMANDELLI_T1:\n",
    "                print(\"\\nAssessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\\n\")\n",
    "            if assessment == ROHRERMANDELLI_T2:\n",
    "                print(\"\\nAssessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\\n\")\n",
    "\n",
    "            print(\"        Naming   Comprehension  Repetition \\n\")\n",
    "            print(f\"Real:   {self.REAL_DATA[group][NAMING] : 5.2f}         {self.REAL_DATA[group][COMPREHENSION] : 5.2f}        {self.REAL_DATA[group][REPETITION] : 5.2f} \\n\")\n",
    "            print(\"Lesion:                                    MAE\\n\")\n",
    "\n",
    "\n",
    "            for lesion_value in range(N_lesion_values): \n",
    "\n",
    "                self.SIM_DATA[group][NAMING] = (self.MEAN_ACT_ST[lesion_value][group][NAMING] - self.MEAN_ACT_SR[lesion_value][group][NAMING]) / (self.MEAN_ACT_ST[lesion_value][NORMAL][NAMING] - self.MEAN_ACT_SR[lesion_value][NORMAL][NAMING]) * 100.0\n",
    "\n",
    "                self.SIM_DATA[group][COMPREHENSION] = (self.MEAN_ACT_CT[lesion_value][group][COMPREHENSION] - self.MEAN_ACT_CR[lesion_value][group][COMPREHENSION]) / (self.MEAN_ACT_CT[lesion_value][NORMAL][COMPREHENSION] - self.MEAN_ACT_CR[lesion_value][NORMAL][COMPREHENSION]) * 100.0\n",
    "\n",
    "                self.SIM_DATA[group][REPETITION] = (self.MEAN_ACT_ST[lesion_value][group][REPETITION] - self.MEAN_ACT_SR[lesion_value][group][REPETITION]) / (self.MEAN_ACT_ST[lesion_value][NORMAL][REPETITION] - self.MEAN_ACT_SR[lesion_value][NORMAL][REPETITION]) * 100.0\n",
    "\n",
    "                if group == NORMAL:\n",
    "                    LV = 1.0\n",
    "                elif WEIGHT_LESION:\n",
    "                    LV = self.WEIGHT_value[lesion_value]\n",
    "                elif DECAY_LESION:\n",
    "                    LV = self.DECAY_value[lesion_value]\n",
    "\n",
    "                self.GOODNESS_OF_FIT[lesion_value] = (abs(self.REAL_DATA[group][NAMING] - self.SIM_DATA[group][NAMING])\n",
    "                    + abs(self.REAL_DATA[group][COMPREHENSION] - self.SIM_DATA[group][COMPREHENSION])\n",
    "                    + abs(self.REAL_DATA[group][REPETITION] - self.SIM_DATA[group][REPETITION]) ) / 3.0\n",
    "\n",
    "                if SHOW_RESULTS_ALL_VALUES: # toggle for printing the results for all lesion values \n",
    "                    sim_combined = (abs(self.REAL_DATA[group][NAMING] - self.SIM_DATA[group][NAMING]) \n",
    "                                    + abs(self.REAL_DATA[group][COMPREHENSION] - self.SIM_DATA[group][COMPREHENSION]) \n",
    "                                    + abs(self.REAL_DATA[group][REPETITION] - self.SIM_DATA[group][REPETITION])) / 3.0\n",
    "                    \n",
    "                    print(f\"{LV : 5.2f}   {self.SIM_DATA[group][NAMING] : 5.2f}        {self.SIM_DATA[group][COMPREHENSION] : 5.2f}        {self.SIM_DATA[group][REPETITION] : 5.2f}     {sim_combined : 5.2f}\\n\")\n",
    "\n",
    "            a = 0\n",
    "            for i in range(N_lesion_values):\n",
    "                if self.GOODNESS_OF_FIT[a] > self.GOODNESS_OF_FIT[i]:\n",
    "                    a = i\n",
    "\n",
    "\n",
    "            if WEIGHT_LESION:\n",
    "                print(f\"Best fit weight value = {self.WEIGHT_value[a] : .2f}   MAE = {self.GOODNESS_OF_FIT[a] : .2f}\\n\")\n",
    "            if DECAY_LESION:\n",
    "                print(f\"Best fit decay value = {self.DECAY_value[a] : .2f}   MAE = {self.GOODNESS_OF_FIT[a] : .2f}\\n\")\n",
    "\n",
    "                sim_naming = (self.MEAN_ACT_ST[a][group][NAMING] - self.MEAN_ACT_SR[a][group][NAMING]) / (self.MEAN_ACT_ST[a][NORMAL][NAMING] - self.MEAN_ACT_SR[a][NORMAL][NAMING]) * 100.0\n",
    "                sim_comprehension = (self.MEAN_ACT_CT[a][group][COMPREHENSION] - self.MEAN_ACT_CR[a][group][COMPREHENSION]) / (self.MEAN_ACT_CT[a][NORMAL][COMPREHENSION] - self.MEAN_ACT_CR[a][NORMAL][COMPREHENSION]) * 100.0\n",
    "                sim_repetition = (self.MEAN_ACT_ST[a][group][REPETITION] - self.MEAN_ACT_SR[a][group][REPETITION]) / (self.MEAN_ACT_ST[a][NORMAL][REPETITION] - self.MEAN_ACT_SR[a][NORMAL][REPETITION]) * 100.0\n",
    "                    \n",
    "                print(f\"Sim:   {sim_naming : 5.2f}         {sim_comprehension : 5.2f}        {sim_repetition : 5.2f} \\n\")\n",
    "\n",
    "\n",
    "    # MAIN ROUTINES \n",
    "\n",
    "    def main(self):\n",
    "        ls = 0.0 # exact lesion value \n",
    "\n",
    "        self.print_heading()\n",
    "        self.print_parameters()\n",
    "        self.set_spreading_rates()\n",
    "\n",
    "        if WEIGHT_LESION:\n",
    "            ls = 0.0\n",
    "            for lesion_value in range(N_lesion_values):\n",
    "                # values between maximally damaged, 0.0, and minimally damaged, 0.99\n",
    "                self.WEIGHT_value[lesion_value] = ls \n",
    "                ls += 0.01\n",
    "\n",
    "        if DECAY_LESION:\n",
    "             # values between minimally damaged, 1.01, and maximally damaged, i.e., full decay, 1.66\n",
    "            ls = 1.01\n",
    "            for lesion_value in range(N_lesion_values):\n",
    "                self.DECAY_value[lesion_value] = ls\n",
    "                ls += 0.01\n",
    "\n",
    "\n",
    "        for assessment in range(N_ASSESSMENTs):\n",
    "\n",
    "            self.set_real_data_matrix(assessment)\n",
    "\n",
    "            for group in range(N_GROUPs):\n",
    "\n",
    "                for task in range(N_TASKs): \n",
    "\n",
    "                    for lesion_value in range(N_lesion_values): \n",
    "\n",
    "                        self.reset_network()\n",
    "                        self.set_aphasic_parameters(group, lesion_value)\n",
    "\n",
    "                        T = 0 # time in ms\n",
    "                        step = 0\n",
    "                        while T < (N_STEPs * STEP_SIZE):\n",
    "\n",
    "                            self.update_network(task, T)\n",
    "                            self.determine_activation_critical_nodes(lesion_value, step, group, task)\n",
    "\n",
    "                            T += STEP_SIZE\n",
    "                            step += 1\n",
    "\n",
    "            # MAPPING TO HUMAN DATA\n",
    "            self.compute_activation_results()\n",
    "            self.compute_fits_and_print_results_on_screen(assessment)  \n",
    "            input()\n",
    "\n",
    "        return 0\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8c0fb",
   "metadata": {},
   "source": [
    "### Run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b142c8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WEAVER++/ARC model simulation of primary progressive aphasia (c) Ardi Roelofs\n",
      "\n",
      "Simulation of group studies \n",
      "\n",
      "Parameter values:\n",
      "\n",
      "cycle time :     25 [ms]\n",
      "\n",
      "sem_rate   :  0.0101 [prop/ms]\n",
      "\n",
      "lem_rate   :  0.0074 [prop/ms]\n",
      "\n",
      "exin       :  0.1965 [act_units/ms]\n",
      "\n",
      "d          :  0.0240 [prop/ms]\n",
      "\n",
      "\n",
      "Assessment is Savage et al. (2013), English\n",
      "\n",
      " \n",
      "\n",
      "NORMAL \n",
      "\n",
      "\n",
      "Savage et al. (2013), English\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    88.70          97.00         99.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.01   MAE =  4.87\n",
      "\n",
      "Sim:    100.00          100.00         100.00 \n",
      "\n",
      " \n",
      "\n",
      "NONFLUENT/AGRAMMATIC \n",
      "\n",
      "\n",
      "Savage et al. (2013), English\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    78.30          94.30         79.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.19   MAE =  0.57\n",
      "\n",
      "Sim:    79.73          94.11         79.60 \n",
      "\n",
      " \n",
      "\n",
      "SEMANTIC DEMENTIA  \n",
      "\n",
      "\n",
      "Savage et al. (2013), English\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    22.70          63.30         95.30 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.77   MAE =  4.47\n",
      "\n",
      "Sim:    13.47          63.25         99.42 \n",
      "\n",
      " \n",
      "\n",
      "LOGOPENIC  \n",
      "\n",
      "\n",
      "Savage et al. (2013), English\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    41.30          84.70         84.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  2.00   MAE =  11.59\n",
      "\n",
      "Sim:    49.81          97.84         97.82 \n",
      "\n",
      "\n",
      "\n",
      "Assessment is Janssen et al. (2022), Dutch\n",
      "\n",
      " \n",
      "\n",
      "NORMAL \n",
      "\n",
      "\n",
      "Assessment is Janssen et al. (2022), Dutch\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    90.30          96.30         96.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.01   MAE =  5.57\n",
      "\n",
      "Sim:    100.00          100.00         100.00 \n",
      "\n",
      " \n",
      "\n",
      "NONFLUENT/AGRAMMATIC \n",
      "\n",
      "\n",
      "Assessment is Janssen et al. (2022), Dutch\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    77.30          97.70         89.30 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.09   MAE =  4.30\n",
      "\n",
      "Sim:    89.25          96.86         89.18 \n",
      "\n",
      " \n",
      "\n",
      "SEMANTIC DEMENTIA  \n",
      "\n",
      "\n",
      "Assessment is Janssen et al. (2022), Dutch\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    29.00          78.00         96.30 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.24   MAE =  3.42\n",
      "\n",
      "Sim:    28.63          84.67         99.51 \n",
      "\n",
      " \n",
      "\n",
      "LOGOPENIC  \n",
      "\n",
      "\n",
      "Assessment is Janssen et al. (2022), Dutch\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    66.30          93.70         91.30 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.50   MAE =  4.10\n",
      "\n",
      "Sim:    66.50          98.55         98.54 \n",
      "\n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), baseline T1\n",
      "\n",
      " \n",
      "\n",
      "NORMAL \n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), baseline T1\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    90.30          96.30         96.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.01   MAE =  5.57\n",
      "\n",
      "Sim:    100.00          100.00         100.00 \n",
      "\n",
      " \n",
      "\n",
      "NONFLUENT/AGRAMMATIC \n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), baseline T1\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    85.30          99.70         83.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.13   MAE =  1.84\n",
      "\n",
      "Sim:    85.19          95.68         85.08 \n",
      "\n",
      " \n",
      "\n",
      "SEMANTIC DEMENTIA  \n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), baseline T1\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    26.70          88.00         90.60 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.26   MAE =  4.62\n",
      "\n",
      "Sim:    27.25          83.60         99.50 \n",
      "\n",
      " \n",
      "\n",
      "LOGOPENIC  \n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), baseline T1\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    69.30          95.00         69.00 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.44   MAE =  11.12\n",
      "\n",
      "Sim:    69.29          98.67         98.66 \n",
      "\n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), follow up T2\n",
      "\n",
      " \n",
      "\n",
      "NORMAL \n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), follow up T2\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    90.30          96.30         96.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.01   MAE =  5.57\n",
      "\n",
      "Sim:    100.00          100.00         100.00 \n",
      "\n",
      " \n",
      "\n",
      "NONFLUENT/AGRAMMATIC \n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), follow up T2\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    83.30          94.80         68.00 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.16   MAE =  5.08\n",
      "\n",
      "Sim:    82.37          94.87         82.25 \n",
      "\n",
      " \n",
      "\n",
      "SEMANTIC DEMENTIA  \n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), follow up T2\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    19.30          66.70         82.30 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.66   MAE =  7.18\n",
      "\n",
      "Sim:    14.95          66.75         99.43 \n",
      "\n",
      " \n",
      "\n",
      "LOGOPENIC  \n",
      "\n",
      "\n",
      "Assessment is Brambati et al. (2015), follow up T2\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    52.70          95.00         58.80 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.89   MAE =  14.04\n",
      "\n",
      "Sim:    52.72          97.96         97.95 \n",
      "\n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\n",
      "\n",
      " \n",
      "\n",
      "NORMAL \n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    90.30          96.30         96.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.01   MAE =  5.57\n",
      "\n",
      "Sim:    100.00          100.00         100.00 \n",
      "\n",
      " \n",
      "\n",
      "NONFLUENT/AGRAMMATIC \n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    76.70          99.00         81.50 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.17   MAE =  3.10\n",
      "\n",
      "Sim:    81.47          94.61         81.35 \n",
      "\n",
      " \n",
      "\n",
      "SEMANTIC DEMENTIA  \n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    26.70          88.00         90.60 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.26   MAE =  4.62\n",
      "\n",
      "Sim:    27.25          83.60         99.50 \n",
      "\n",
      " \n",
      "\n",
      "LOGOPENIC  \n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    61.00          94.00         94.00 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.64   MAE =  2.93\n",
      "\n",
      "Sim:    60.80          98.31         98.30 \n",
      "\n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\n",
      "\n",
      " \n",
      "\n",
      "NORMAL \n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    90.30          96.30         96.70 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.01   MAE =  5.57\n",
      "\n",
      "Sim:    100.00          100.00         100.00 \n",
      "\n",
      " \n",
      "\n",
      "NONFLUENT/AGRAMMATIC \n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    66.00          90.00         65.50 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.39   MAE =  0.14\n",
      "\n",
      "Sim:    65.71          90.09         65.54 \n",
      "\n",
      " \n",
      "\n",
      "SEMANTIC DEMENTIA  \n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    26.70          88.00         90.60 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  1.26   MAE =  4.62\n",
      "\n",
      "Sim:    27.25          83.60         99.50 \n",
      "\n",
      " \n",
      "\n",
      "LOGOPENIC  \n",
      "\n",
      "\n",
      "Assessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\n",
      "\n",
      "        Naming   Comprehension  Repetition \n",
      "\n",
      "Real:    43.00          85.00         77.00 \n",
      "\n",
      "Lesion:                                    MAE\n",
      "\n",
      "Best fit decay value =  2.00   MAE =  13.49\n",
      "\n",
      "Sim:    49.81          97.84         97.82 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model()\n",
    "m.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab2bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
