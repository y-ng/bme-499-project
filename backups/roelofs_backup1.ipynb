{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be2be71",
   "metadata": {},
   "source": [
    "# Recreating Roelof's PPA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1a6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as pyplot \n",
    "\n",
    "STEP_SIZE = 25   # duration time step in ms\n",
    "N_STEPs = 80     # 2000 ms in total\n",
    "N_CONCEPTs = 5   \n",
    "N_LEMMAs = 5     \n",
    "N_MORPHEMEs = 5  \n",
    "N_PHONEMEs = 10   \n",
    "N_SYLLABLEs = 5  \n",
    "\n",
    "N_lesion_values = 100 # for 100 for weight lesion, 66 (!) for decay lesion\n",
    "\n",
    "N_GROUPs = 4 # Normal, Nonfluent_agrammatic, Semantic_dementia, Logopenic\n",
    "NORMAL = 0\n",
    "NONFLUENT_AGRAMMATIC = 1\n",
    "SEMANTIC_DEMENTIA = 2\n",
    "LOGOPENIC = 3\n",
    "\n",
    "N_TASKs = 3 # Naming, Comprehension, Repetition\n",
    "NAMING = 0\n",
    "COMPREHENSION = 1\n",
    "REPETITION = 2\n",
    "\n",
    "N_ASSESSMENTs = 6\n",
    "ENGLISH = 0\n",
    "DUTCH = 1\n",
    "BRAMBATI_T1 = 2 # baseline \n",
    "BRAMBATI_T2 = 3 # follow up \n",
    "ROHRERMANDELLI_T1 = 4 # baseline \n",
    "ROHRERMANDELLI_T2 = 5 # follow up \n",
    "\n",
    "Y = 1.0     # connection present \n",
    "N = 0.0     # connection absent\n",
    "\n",
    "# labeling network nodes\n",
    "CAT = 0\n",
    "DOG = 1\n",
    "MAT = 2\n",
    "FOG = 3\n",
    "FISH = 4\n",
    "\n",
    "pK = 0 # phonemes\n",
    "pE = 1\n",
    "pT = 2\n",
    "pD = 3\n",
    "pO = 4\n",
    "pG = 5\n",
    "pM = 6\n",
    "pF = 7\n",
    "pI = 8\n",
    "pS = 9\n",
    "\n",
    "Cat = 0\n",
    "Dog = 1\n",
    "Mat = 2\n",
    "Fog = 3\n",
    "Fish = 4\n",
    "\n",
    "# connections conceptual network [N_CONCEPTs][N_CONCEPTs]\n",
    "CC_con =  np.array([\n",
    "    # CAT   DOG  MAT  FOG  FISH  \n",
    "    [   N,    Y,   N,   N,    Y ], # CAT\n",
    "    [   Y,    N,   N,   N,    Y ], # DOG\n",
    "    [   N,    N,   N,   N,    N ], # MAT\n",
    "    [   N,    N,   N,   N,    N ], # FOG\n",
    "    [   Y,    Y,   N,   N,    N ]  # FISH\n",
    "])\n",
    "\n",
    "# connections between concept and lemma nodes [N_CONCEPTs][N_LEMMAs]\n",
    "CL_con = np.array([\n",
    "    [ Y,  N,  N,  N,  N ],\n",
    "    [ N,  Y,  N,  N,  N ],\n",
    "    [ N,  N,  Y,  N,  N ],\n",
    "    [ N,  N,  N,  Y,  N ],\n",
    "    [ N,  N,  N,  N,  Y ]\n",
    "])\n",
    "\n",
    "# connections between lemma nodes and morpheme nodes [N_LEMMAs][N_MORPHEMEs]\n",
    "LM_con = np.array([\n",
    "    [ Y,  N,  N,  N,  N ],\n",
    "    [ N,  Y,  N,  N,  N ],\n",
    "    [ N,  N,  Y,  N,  N ],\n",
    "    [ N,  N,  N,  Y,  N ],\n",
    "    [ N,  N,  N,  N,  Y ]\n",
    "])\n",
    "\n",
    "# connections between morpheme nodes and output phoneme nodes [N_MORPHEMEs][N_PHONEMEs]\n",
    "MP_con = np.array([\n",
    "     #  K  E  T  D  O  G  M  F  I  S  \n",
    "    [   Y, Y, Y, N, N, N, N, N, N, N ], # <cat>\n",
    "    [   N, N, N, Y, Y, Y, N, N, N, N ], # <dog>\n",
    "    [   N, Y, Y, N, N, N, Y, N, N, N ], # <mat>\n",
    "    [   N, N, N, N, Y, Y, N, Y, N, N ], # <fog>\n",
    "    [   N, N, N, N, N, N, N, Y, Y, Y ]  # <fish>\n",
    "])\n",
    "\n",
    "# connections between output phoneme nodes and syllable program nodes [N_PHONEMEs][N_SYLLABLEs]\n",
    "PS_con = np.array([\n",
    "    # Cat Dog  Mat Fog  Fish\n",
    "    [ Y,   N,   N,  N,   N ], # K\n",
    "    [ Y,   N,   Y,  N,   N ], # E\n",
    "    [ Y,   N,   Y,  N,   N ], # T\n",
    "    [ N,   Y,   N,  N,   N ], # D\n",
    "    [ N,   Y,   N,  Y,   N ], # O\n",
    "    [ N,   Y,   N,  Y,   N ], # G\n",
    "    [ N,   N,   Y,  N,   N ], # M\n",
    "    [ N,   N,   N,  Y,   Y ], # F\n",
    "    [ N,   N,   N,  N,   Y ], # I\n",
    "    [ N,   N,   N,  N,   Y ]  # S\n",
    "])\n",
    "\n",
    "# connections between input and output phoneme nodes [N_PHONEMEs][N_PHONEMEs]\n",
    "PP_con = np.array([\n",
    "    # K    E    T   D    O    G   M   F   I  S \n",
    "    [ Y,   N,   N,  N,   N,   N,  N,  N,  N, N  ], # K\n",
    "    [ N,   Y,   N,  N,   N,   N,  N,  N,  N, N  ], # E\n",
    "    [ N,   N,   Y,  N,   N,   N,  N,  N,  N, N  ], # T\n",
    "    [ N,   N,   N,  Y,   N,   N,  N,  N,  N, N  ], # D\n",
    "    [ N,   N,   N,  N,   Y,   N,  N,  N,  N, N  ], # O\n",
    "    [ N,   N,   N,  N,   N,   Y,  N,  N,  N, N  ], # G\n",
    "    [ N,   N,   N,  N,   N,   N,  Y,  N,  N, N  ], # M\n",
    "    [ N,   N,   N,  N,   N,   N,  N,  Y,  N, N  ], # F\n",
    "    [ N,   N,   N,  N,   N,   N,  N,  N,  Y, N  ], # I\n",
    "    [ N,   N,   N,  N,   N,   N,  N,  N,  N, Y  ]  # S\n",
    "])\n",
    "\n",
    "# connections between input phoneme nodes and input morpheme nodes [N_PHONEMEs][N_MORPHEMEs]\n",
    "PiM_con = np.array([\n",
    "    # Cat Dog  Mat Fog  Fish \n",
    "    [ Y,   N,   N,  N,   N ], # K\n",
    "    [ Y,   N,   Y,  N,   N ], # E\n",
    "    [ Y,   N,   Y,  N,   N ], # T\n",
    "    [ N,   Y,   N,  N,   N ], # D\n",
    "    [ N,   Y,   N,  Y,   N ], # O\n",
    "    [ N,   Y,   N,  Y,   N ], # G\n",
    "    [ N,   N,   Y,  N,   N ], # M\n",
    "    [ N,   N,   N,  Y,   Y ], # F\n",
    "    [ N,   N,   N,  N,   Y ], # I\n",
    "    [ N,   N,   N,  N,   Y ]  # S\n",
    "])\n",
    "\n",
    "# connections between input morpheme and output morpheme nodes [N_MORPHEMEs][N_MORPHEMEs]\n",
    "iMM_con = np.array([\n",
    "    [ Y,  N,  N,  N,  N ],\n",
    "    [ N,  Y,  N,  N,  N ],\n",
    "    [ N,  N,  Y,  N,  N ],\n",
    "    [ N,  N,  N,  Y,  N ],\n",
    "    [ N,  N,  N,  N,  Y ]\n",
    "])\n",
    "\n",
    "# connections between input morpheme and lemma nodes [N_MORPHEMEs][N_LEMMAs]\n",
    "iML_con = np.array([\n",
    "    [ Y,  N,  N,  N,  N ],\n",
    "    [ N,  Y,  N,  N,  N ],\n",
    "    [ N,  N,  Y,  N,  N ],\n",
    "    [ N,  N,  N,  Y,  N ],\n",
    "    [ N,  N,  N,  N,  Y ]\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf84543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English data on PPA for single word tasks: Savage et al. (2013) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_ENGLISH = np.array([\n",
    "    # Naming  Comprehension Repetition\n",
    "    [ 88.7,      97.0,      99.7 ], # Control\n",
    "    [ 78.3,      94.3,      79.7 ], # nfvPPA\n",
    "    [ 22.7,      63.3,      95.3 ], # svPPA\n",
    "    [ 41.3,      84.7,      84.7 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# Dutch data on PPA for single word tasks: Janssen et al. (2021) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_DUTCH = np.array([\n",
    "    # Naming  Comprehension Repetition */\n",
    "    [ 90.3,      96.3,      96.7 ], # Control\n",
    "    [ 77.3,      97.7,      89.3 ], # nfvPPA\n",
    "    [ 29.0,      78.0,      96.3 ], # svPPA\n",
    "    [ 66.3,      93.7,      91.3 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# Brambati T1 data on PPA for single word tasks: Brambati et al. (2015) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_BRAMBATI_T1 = np.array([\n",
    "    # Naming  Comprehension Repetition \n",
    "    [ 90.3,      96.3,      96.7 ], # Control (dummy, from Savage)\n",
    "    [ 85.3,      99.7,      83.7 ], # nfvPPA\n",
    "    [ 26.7,      88.0,      90.6 ], # svPPA\n",
    "    [ 69.3,      95.0,      69.0 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# Brambati T2 data on PPA for single word tasks: Brambati et al. (2015) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_BRAMBATI_T2 = np.array([\n",
    "    # Naming  Comprehension Repetition\n",
    "    [ 90.3,      96.3,      96.7 ], # Control (dummy, from Savage)\n",
    "    [ 83.3,      94.8,      68.0 ], # nfvPPA\n",
    "    [ 19.3,      66.7,      82.3 ], # svPPA\n",
    "    [ 52.7,      95.0,      58.8 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# Rohrer et al. (2013), logopenic patients (N=21), T1 baseline and T2 one year later\n",
    "# Mandelli et al. (2016), nonfluent/agrammatic patients (N=34), T1 baseline and T2 one year later\n",
    "\n",
    "\n",
    "# RohrerMandelli T1 data on PPA: Rohrer et al. (2013), Mandelli et al. (2016) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_ROHRERMANDELLI_T1 = np.array([\n",
    "    # Naming  Comprehension Repetition \n",
    "    [ 90.3,      96.3,      96.7 ], # Control (dummy, from Savage)\n",
    "    [ 76.7,      99.0,      81.5 ], # nfvPPA\n",
    "    [ 26.7,      88.0,      90.6 ], # svPPA (dummy, from Brambati)\n",
    "    [ 61.0,      94.0,      94.0 ]  # lvPPA\n",
    "])\n",
    "\n",
    "# RohrerMandelli T2 data on PPA: Rohrer et al. (2013), Mandelli et al. (2016) [N_GROUPs][N_TASKs]\n",
    "REAL_DATA_ROHRERMANDELLI_T2 = np.array([\n",
    "    # Naming  Comprehension Repetition \n",
    "    [ 90.3,      96.3,      96.7 ], # Control (dummy, from Savage)\n",
    "    [ 66.0,      90.0,      65.5 ], # nfvPPA\n",
    "    [ 26.7,      88.0,      90.6 ], # svPPA (dummy, from Brambati)\n",
    "    [ 43.0,      85.0,      77.0 ]  # lvPPA\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74fd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_DATA = np.zeros(shape=(N_GROUPs, N_TASKs))\n",
    "SIM_DATA = np.zeros(shape=(N_GROUPs, N_TASKs))\n",
    "GOODNESS_OF_FIT = np.zeros(N_lesion_values)\n",
    "\n",
    "WEIGHT_value = np.zeros(N_lesion_values)\n",
    "DECAY_value = np.zeros(N_lesion_values)\n",
    "\n",
    "# concept and lemma\n",
    "C_node_act = np.zeros(N_CONCEPTs)\n",
    "L_node_act = np.zeros(N_LEMMAs)\n",
    "# output form \n",
    "M_node_act = np.zeros(N_MORPHEMEs)\n",
    "oP_node_act = np.zeros(N_PHONEMEs)\n",
    "S_node_act = np.zeros(N_SYLLABLEs)\n",
    "# input form \n",
    "iM_node_act = np.zeros(N_MORPHEMEs)\n",
    "iP_node_act = np.zeros(N_PHONEMEs)\n",
    "\n",
    "# input buffer for nodes \n",
    "input_C = np.zeros(N_CONCEPTs)\n",
    "input_L = np.zeros(N_LEMMAs)\n",
    "input_M = np.zeros(N_MORPHEMEs)\n",
    "input_iM = np.zeros(N_MORPHEMEs)\n",
    "input_iP = np.zeros(N_PHONEMEs)\n",
    "input_oP = np.zeros(N_PHONEMEs)\n",
    "input_S = np.zeros(N_SYLLABLEs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532b5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter values\n",
    "CYCLE_TIME = 25                 # ms per link \n",
    "SEM_rate = 0.0101 * STEP_SIZE   # prop per step_size ms \n",
    "LEM_rate = 0.0074 * STEP_SIZE   # prop per step_size ms \n",
    "LEX_rate = 0.0120 * STEP_SIZE   # prop per step_size ms \n",
    "DECAY_rate = 0.0240 * STEP_SIZE # prop per step_size ms \n",
    "EXTIN = 0.1965 * STEP_SIZE      # act_units per step_size ms \n",
    "LEMLEXFRAC = 0.3 \n",
    "# fraction of LEX_rate spread between lemmas and output morphemes \n",
    "# implementing weak cascading of activation, see Roelofs (2008, JEP:LMC) \n",
    "\n",
    "FR = 0.10  # fraction of connection weight for input phoneme to input morpheme, cf. Roelofs (1997, Cognition)\n",
    "SEGMENT_DURATION = 125  # ms\n",
    "PICTURE_DURATION = 125  # ms\n",
    "\n",
    "# set here to simulate weight or decay lesion and what to print \n",
    "WEIGHT_LESION = 0 #1\n",
    "DECAY_LESION = 1 #0\n",
    "\n",
    "SHOW_RESULTS_ALL_VALUES = 0 # set here whether to print all values \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131eaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aphasia parameters\n",
    "\n",
    "# weight lesion \n",
    "CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC = 0.0 # connections to and from output phonemes \n",
    "CONNECTION_DECREASE_SEMANTIC_DEMENTIA = 0.0 # connections to, within, and from conceptual network\n",
    "CONNECTION_DECREASE_LOGOPENIC = 0.0 # connections to and from lexical output forms, and between input and output phonemes\n",
    "\n",
    "# decay lesion\n",
    "DECAY_INCREASE_NONFLUENT_AGRAMMATIC = 0.0 # output phonemes \n",
    "DECAY_INCREASE_SEMANTIC_DEMENTIA = 0.0 # concepts\n",
    "DECAY_INCREASE_LOGOPENIC = 0.0 # lexical output forms\n",
    "\n",
    "ACT_C = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "ACT_S = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "\n",
    "# Activation of target concept, cat\n",
    "ACT_CT = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "# Activation of conceptual relative, dog\n",
    "ACT_CR= np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "\n",
    "# Activation of target lemma, cat\n",
    "ACT_LT = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "# Activation of lemma relative, i.e., semantically related, dog\n",
    "ACT_LR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "\n",
    "# Activation of target syllable, cat\n",
    "ACT_ST = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "# Activation of syllabic relative, mat\n",
    "ACT_SR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "\n",
    "TOTAL_ACT_C = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "MEAN_ACT_C = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "TOTAL_ACT_S = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "MEAN_ACT_S = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "\n",
    "# T = target, R = relative\n",
    "TOTAL_ACT_CT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "MEAN_ACT_CT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "TOTAL_ACT_CR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "MEAN_ACT_CR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "\n",
    "TOTAL_ACT_LT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "MEAN_ACT_LT = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "TOTAL_ACT_LR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "MEAN_ACT_LR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "\n",
    "TOTAL_ACT_ST = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "MEAN_ACT_ST = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "TOTAL_ACT_SR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "MEAN_ACT_SR = np.zeros(shape=(N_lesion_values, N_GROUPs, N_TASKs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fbd0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_real_data_matrix(assessment, REAL_DATA = REAL_DATA):\n",
    "    for group in range(N_GROUPs):\n",
    "        for task in range(N_TASKs):\n",
    "            if assessment == ENGLISH:\n",
    "                REAL_DATA[group][task] = REAL_DATA_ENGLISH[group][task]\n",
    "            elif assessment == DUTCH:\n",
    "                REAL_DATA[group][task] = REAL_DATA_DUTCH[group][task]\n",
    "            elif assessment == BRAMBATI_T1:\n",
    "                REAL_DATA[group][task] = REAL_DATA_BRAMBATI_T1[group][task]\n",
    "            elif assessment == BRAMBATI_T2:\n",
    "                REAL_DATA[group][task] = REAL_DATA_BRAMBATI_T2[group][task]\n",
    "            elif assessment == ROHRERMANDELLI_T1:\n",
    "                REAL_DATA[group][task] = REAL_DATA_ROHRERMANDELLI_T1[group][task]\n",
    "            elif (assessment == ROHRERMANDELLI_T2):\n",
    "                REAL_DATA[group][task] = REAL_DATA_ROHRERMANDELLI_T2[group][task]\n",
    "\n",
    "                \n",
    "def set_spreading_rates(\n",
    "    ACT_C = ACT_C, \n",
    "    ACT_S = ACT_S, \n",
    "    ACT_CT = ACT_CT, \n",
    "    ACT_CR = ACT_CR,\n",
    "    ACT_LT = ACT_LT,\n",
    "    ACT_LR = ACT_LR,\n",
    "    ACT_ST = ACT_ST,\n",
    "    ACT_SR = ACT_SR,\n",
    "    CC_con = CC_con,\n",
    "    CL_con = CL_con,\n",
    "    LM_con = LM_con,\n",
    "    MP_con = MP_con,\n",
    "    PS_con = PS_con,\n",
    "    PP_con = PP_con,\n",
    "    PiM_con = PiM_con,\n",
    "    iMM_con = iMM_con,\n",
    "    iML_con = iML_con,\n",
    "):\n",
    "   \n",
    "    ACT_C = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "    ACT_S = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "    ACT_CT = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "    ACT_CR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "    ACT_LT = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "    ACT_LR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "    ACT_ST = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "    ACT_SR = np.zeros(shape=(N_lesion_values, N_STEPs, N_GROUPs, N_TASKs))\n",
    "    \n",
    "    \"\"\"\n",
    "    for lesion_value in range(N_lesion_values):\n",
    "        for group in range(N_GROUPs):\n",
    "            for task in range(N_TASKs):\n",
    "                for step in range(N_STEPs):\n",
    "                    ACT_C[lesion_value][step][group][task] = 0.0\n",
    "                    ACT_S[lesion_value][step][group][task] = 0.0\n",
    "                    ACT_CT[lesion_value][step][group][task] = 0.0\n",
    "                    ACT_CR[lesion_value][step][group][task] = 0.0\n",
    "                    ACT_LT[lesion_value][step][group][task] = 0.0\n",
    "                    ACT_LR[lesion_value][step][group][task] = 0.0\n",
    "                    ACT_ST[lesion_value][step][group][task] = 0.0\n",
    "                    ACT_SR[lesion_value][step][group][task] = 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    CC_con *= SEM_rate\n",
    "    CL_con *= LEM_rate\n",
    "    LM_con *= LEX_rate\n",
    "    MP_con *= LEX_rate\n",
    "    PS_con *= LEX_rate\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(N_CONCEPTs):\n",
    "        for j in range(N_CONCEPTs):\n",
    "            CC_con[i][j]*=SEM_rate\n",
    "\n",
    "    for i in range(N_CONCEPTs):\n",
    "        for j in range(N_LEMMAs):\n",
    "            CL_con[i][j]*=LEM_rate\n",
    "\n",
    "\n",
    "    for i in range(N_LEMMAs):\n",
    "        for j in range(N_MORPHEMEs):\n",
    "            LM_con[i][j]*=LEX_rate\n",
    "\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        for j in range(N_PHONEMEs):\n",
    "            MP_con[i][j]*=LEX_rate\n",
    "\n",
    "    for i in range(N_PHONEMEs):\n",
    "        for j in range(N_SYLLABLEs):\n",
    "            PS_con[i][j]*=LEX_rate\n",
    "    \"\"\"\n",
    "\n",
    "    # connections for input phonemes to output phonemes, input morphemes, input morphemes to lemmas\n",
    "            \n",
    "    PP_con *= LEX_rate\n",
    "    PiM_con *= (FR * LEX_rate)\n",
    "    iMM_con *= LEX_rate\n",
    "    iML_con *= LEX_rate\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(N_PHONEMEs):\n",
    "        for j in range(N_PHONEMEs):\n",
    "            PP_con[i][j]*=LEX_rate\n",
    "\n",
    "    for i in range(N_PHONEMEs):\n",
    "        for j in range(N_MORPHEMEs):\n",
    "            PiM_con[i][j]*= (FR * LEX_rate)\n",
    "\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        for j in range(N_MORPHEMEs): \n",
    "            iMM_con[i][j]*=LEX_rate\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        for j in range(N_LEMMAs):\n",
    "            iML_con[i][j]*=LEX_rate\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def reset_network():\n",
    "\n",
    "    C_node_act = np.zeros(N_CONCEPTs)\n",
    "    L_node_act = np.zeros(N_LEMMAs)\n",
    "    M_node_act = np.zeros(N_MORPHEMEs)\n",
    "    iM_node_act = np.zeros(N_MORPHEMEs)\n",
    "    iP_node_act = np.zeros(N_PHONEMEs)\n",
    "    oP_node_act = np.zeros(N_PHONEMEs)\n",
    "    S_node_act = np.zeros(N_SYLLABLEs)\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(N_CONCEPTs):\n",
    "        C_node_act[i]=0.0\n",
    "\n",
    "    for i in range(N_LEMMAs):\n",
    "        L_node_act[i]=0.0\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        M_node_act[i]=0.0\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        iM_node_act[i]=0.0\n",
    "  \n",
    "    for i in range(N_PHONEMEs):\n",
    "        iP_node_act[i]=0.0\n",
    "\n",
    "    for i in range(N_PHONEMEs):\n",
    "        oP_node_act[i]=0.0\n",
    "   \n",
    "    for i in range(N_SYLLABLEs):\n",
    "        S_node_act[i]=0.0\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "def set_aphasic_parameters(group, lesion_value):\n",
    "\n",
    "    WEIGHT_FACTOR, DECAY_FACTOR = 0.0, 0.0\n",
    "\n",
    "    if WEIGHT_LESION:\n",
    "        WEIGHT_FACTOR = WEIGHT_value[lesion_value]  \n",
    "    else:\n",
    "        WEIGHT_FACTOR = 1.0\n",
    "\n",
    "    if DECAY_LESION:\n",
    "        DECAY_FACTOR = DECAY_value[lesion_value]   \n",
    "    else:\n",
    "        DECAY_FACTOR = 1.0\n",
    "\n",
    "\n",
    "    # setting of weight parameters\n",
    "\n",
    "    if group == NONFLUENT_AGRAMMATIC:\n",
    "        CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC = WEIGHT_FACTOR\n",
    "    else:\n",
    "        CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC = 1.0 # normal \n",
    "\n",
    "    if group == SEMANTIC_DEMENTIA:\n",
    "        CONNECTION_DECREASE_SEMANTIC_DEMENTIA = WEIGHT_FACTOR\n",
    "    else:\n",
    "        CONNECTION_DECREASE_SEMANTIC_DEMENTIA = 1.0 # normal\n",
    "  \n",
    "    if group == LOGOPENIC:\n",
    "        CONNECTION_DECREASE_LOGOPENIC = WEIGHT_FACTOR\n",
    "    else:\n",
    "        CONNECTION_DECREASE_LOGOPENIC = 1.0 # normal\n",
    "\n",
    "    # setting of decay parameters\n",
    "\n",
    "    if group == NONFLUENT_AGRAMMATIC:\n",
    "        DECAY_INCREASE_NONFLUENT_AGRAMMATIC = DECAY_FACTOR\n",
    "    else:\n",
    "        DECAY_INCREASE_NONFLUENT_AGRAMMATIC = 1.0 # normal\n",
    "\n",
    "    if group == SEMANTIC_DEMENTIA:\n",
    "        DECAY_INCREASE_SEMANTIC_DEMENTIA = DECAY_FACTOR\n",
    "    else:\n",
    "        DECAY_INCREASE_SEMANTIC_DEMENTIA = 1.0 # normal\n",
    "\n",
    "    if group == LOGOPENIC:\n",
    "        DECAY_INCREASE_LOGOPENIC = DECAY_FACTOR\n",
    "    else:\n",
    "        DECAY_INCREASE_LOGOPENIC = 1.0 # normal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f063d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK UPDATING ROUTINES \n",
    "\n",
    "def update_network(task, T):\n",
    "    set_input_to_zero()\n",
    "    get_external_input(task, T)\n",
    "    get_internal_input()\n",
    "    update_activation_of_nodes()\n",
    "\n",
    "\n",
    "def set_input_to_zero():\n",
    "\n",
    "    for i in range(N_CONCEPTs):\n",
    "        input_C[i]=0.0\n",
    "\n",
    "    for i in range(N_LEMMAs):\n",
    "        input_L[i]=0.0\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        input_M[i]=0.0\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        input_iM[i]=0.0\n",
    "   \n",
    "    for i in range(N_PHONEMEs):\n",
    "        input_iP[i]=0.0\n",
    "\n",
    "    for i in range(N_PHONEMEs):\n",
    "        input_oP[i]=0.0\n",
    "\n",
    "    for i in range(N_SYLLABLEs):\n",
    "        input_S[i]=0.0\n",
    "\n",
    "\n",
    "def get_external_input(task, T):\n",
    "\n",
    "    if task == NAMING:\n",
    "        # picture input\n",
    "        if (T >= 0 and T < PICTURE_DURATION):\n",
    "            input_C[CAT] += CONNECTION_DECREASE_SEMANTIC_DEMENTIA * EXTIN\n",
    "\n",
    "        # enhancement\n",
    "        if ((T >= (0 + 1 * CYCLE_TIME)) and  (T < (1 * CYCLE_TIME + PICTURE_DURATION))):\n",
    "            input_C[CAT]+= EXTIN\n",
    "\n",
    "    if (task == COMPREHENSION or task == REPETITION): \n",
    " \n",
    "        # spoken word input */\n",
    "        if ((0 <= T) and (T < SEGMENT_DURATION)):\n",
    "            input_iP[pK]+=EXTIN\n",
    "\n",
    "        if ((SEGMENT_DURATION <= T) and (T < (2 * SEGMENT_DURATION))):\n",
    "            input_iP[pE]+=EXTIN\n",
    "\n",
    "\n",
    "        if ((2 * SEGMENT_DURATION <= T) and (T < (3 * SEGMENT_DURATION))): \n",
    "            input_iP[pT]+=EXTIN\n",
    "\n",
    "\n",
    "def get_internal_input():\n",
    "\n",
    "    # input activation for concept nodes\n",
    "\n",
    "    for i in range(N_CONCEPTs):\n",
    "        for j in range(N_CONCEPTs):\n",
    "            input_C[i]+=(C_node_act[j] * (CC_con[j][i] * CONNECTION_DECREASE_SEMANTIC_DEMENTIA))\n",
    "\n",
    "    for i in range(N_CONCEPTs):\n",
    "        for j in range(N_LEMMAs):\n",
    "            input_C[i]+=(L_node_act[j] * CL_con[j][i] * CONNECTION_DECREASE_SEMANTIC_DEMENTIA)\n",
    "\n",
    "\n",
    "    # input activation for lemma nodes\n",
    "    for i in range(N_LEMMAs):\n",
    "        for j in range(N_CONCEPTs):\n",
    "            input_L[i]+=( C_node_act[j] * CL_con[j][i] *  CONNECTION_DECREASE_SEMANTIC_DEMENTIA)\n",
    "\n",
    "    for i in range(N_LEMMAs):\n",
    "        for j in range(N_MORPHEMEs):                 \n",
    "            input_L[i]+=( iM_node_act[j] * iML_con[j][i])\n",
    "\n",
    "\n",
    "    # input activation for output morpheme nodes\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        for j in range(N_LEMMAs):\n",
    "            input_M[i]+=( L_node_act[j] * LEMLEXFRAC * LM_con[j][i] * CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        for j in range(N_MORPHEMEs):\n",
    "            input_M[i]+=( iM_node_act[j] * iMM_con[j][i] * CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "    # input activation for output phoneme nodes\n",
    "    for i in range(N_PHONEMEs):\n",
    "        for j in range(N_MORPHEMEs):\n",
    "            input_oP[i]+=( M_node_act[j] * MP_con[j][i] * CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC \n",
    "                          * CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "    for i in range(N_PHONEMEs):\n",
    "        for j in range(N_PHONEMEs):\n",
    "            input_oP[i] += ( iP_node_act[j] * PP_con[j][i] * CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC \n",
    "                            * CONNECTION_DECREASE_LOGOPENIC)\n",
    "\n",
    "\n",
    "    # input activation for syllable program nodes\n",
    "    for i in range(N_SYLLABLEs):\n",
    "        for j in range(N_PHONEMEs):\n",
    "            input_S[i]+=( oP_node_act[j] * PS_con[j][i] * CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC)\n",
    "\n",
    "    # input activation for input phoneme nodes\n",
    "    for i in range(N_PHONEMEs):\n",
    "        for j in range(N_PHONEMEs):\n",
    "            input_iP[i]+=( oP_node_act[j] * PP_con[j][i] * CONNECTION_DECREASE_NONFLUENT_AGRAMMATIC \n",
    "                          * CONNECTION_DECREASE_LOGOPENIC)\n",
    " \n",
    "    # input activation for input morpheme nodes\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        for j in range(N_PHONEMEs):\n",
    "            input_iM[i]+=( iP_node_act[j] * PiM_con[j][i])\n",
    " \n",
    "    \n",
    "def update_activation_of_nodes():\n",
    "\n",
    "    for i in range(N_CONCEPTs):\n",
    "        C_node_act[i]=((C_node_act[i] * (1.0 - (DECAY_rate * DECAY_INCREASE_SEMANTIC_DEMENTIA)) ) + input_C[i])\n",
    "\n",
    "    for i in range(N_LEMMAs):\n",
    "        L_node_act[i]=((L_node_act[i] * (1.0 - DECAY_rate)) + input_L[i])\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        M_node_act[i]=((M_node_act[i] * (1.0 - (DECAY_rate * DECAY_INCREASE_LOGOPENIC) )) + input_M[i])\n",
    "\n",
    "    for i in range(N_PHONEMEs):\n",
    "        oP_node_act[i]=((oP_node_act[i] * (1.0 - (DECAY_rate * DECAY_INCREASE_NONFLUENT_AGRAMMATIC))) + input_oP[i])\n",
    "       \n",
    "    for i in range(N_PHONEMEs):\n",
    "        iP_node_act[i]=((iP_node_act[i] * (1.0 - DECAY_rate)) + input_iP[i])\n",
    "\n",
    "    for i in range(N_MORPHEMEs):\n",
    "        iM_node_act[i]=((iM_node_act[i] * (1.0 - DECAY_rate)) + input_iM[i])\n",
    "   \n",
    "    for i in range(N_SYLLABLEs):\n",
    "        S_node_act[i]=((S_node_act[i] * (1.0 - DECAY_rate)) + input_S[i])\n",
    "\n",
    "\n",
    "def determine_activation_critical_nodes(lesion_value, step, group, task):\n",
    "    ACT_C[lesion_value][step][group][task] = C_node_act[CAT]\n",
    "    ACT_S[lesion_value][step][group][task] = S_node_act[CAT]\n",
    "    ACT_CT[lesion_value][step][group][task] = C_node_act[CAT]\n",
    "    ACT_CR[lesion_value][step][group][task] = C_node_act[DOG]\n",
    "    ACT_LT[lesion_value][step][group][task] = L_node_act[CAT]\n",
    "    ACT_LR[lesion_value][step][group][task] = L_node_act[DOG]\n",
    "    ACT_ST[lesion_value][step][group][task] = S_node_act[CAT]\n",
    "    ACT_SR[lesion_value][step][group][task] = S_node_act[MAT]\n",
    "\n",
    "\n",
    "def compute_activation_results():\n",
    "\n",
    "    for lesion_value in range(N_lesion_values):\n",
    "        for group in range(N_GROUPs):\n",
    "            for task in range(N_TASKs):\n",
    "                TOTAL_ACT_C[lesion_value][group][task] = 0.0\n",
    "                TOTAL_ACT_S[lesion_value][group][task] = 0.0\n",
    "                MEAN_ACT_C[lesion_value][group][task] = 0.0\n",
    "                MEAN_ACT_S[lesion_value][group][task] = 0.0\n",
    "\n",
    "                TOTAL_ACT_CT[lesion_value][group][task] = 0.0\n",
    "                TOTAL_ACT_CR[lesion_value][group][task] = 0.0\n",
    "                MEAN_ACT_CT[lesion_value][group][task] = 0.0\n",
    "                MEAN_ACT_CR[lesion_value][group][task] = 0.0\n",
    "\n",
    "                TOTAL_ACT_LT[lesion_value][group][task] = 0.0\n",
    "                TOTAL_ACT_LR[lesion_value][group][task] = 0.0\n",
    "                MEAN_ACT_LT[lesion_value][group][task] = 0.0\n",
    "                MEAN_ACT_LR[lesion_value][group][task] = 0.0\n",
    "\n",
    "                TOTAL_ACT_ST[lesion_value][group][task] = 0.0\n",
    "                TOTAL_ACT_SR[lesion_value][group][task] = 0.0\n",
    "                MEAN_ACT_ST[lesion_value][group][task] = 0.0\n",
    "                MEAN_ACT_SR[lesion_value][group][task] = 0.0\n",
    "\n",
    "    for lesion_value in range(N_lesion_values):\n",
    "        for group in range(N_GROUPs):\n",
    "            for task in range(N_TASKs): \n",
    "                for i in range(N_STEPs): \n",
    "                    TOTAL_ACT_C[lesion_value][group][task] += ACT_C[lesion_value][i][group][task]\n",
    "                    TOTAL_ACT_S[lesion_value][group][task] += ACT_S[lesion_value][i][group][task]\n",
    "                    TOTAL_ACT_CT[lesion_value][group][task] += ACT_CT[lesion_value][i][group][task]\n",
    "                    TOTAL_ACT_CR[lesion_value][group][task] += ACT_CR[lesion_value][i][group][task]\n",
    "                    TOTAL_ACT_LT[lesion_value][group][task] += ACT_LT[lesion_value][i][group][task]\n",
    "                    TOTAL_ACT_LR[lesion_value][group][task] += ACT_LR[lesion_value][i][group][task]\n",
    "                    TOTAL_ACT_ST[lesion_value][group][task] += ACT_ST[lesion_value][i][group][task]\n",
    "                    TOTAL_ACT_SR[lesion_value][group][task] += ACT_SR[lesion_value][i][group][task]\n",
    "\n",
    "                MEAN_ACT_C[lesion_value][group][task] = (TOTAL_ACT_C[lesion_value][group][task] / N_STEPs)\n",
    "                MEAN_ACT_S[lesion_value][group][task] = (TOTAL_ACT_S[lesion_value][group][task] / N_STEPs)\n",
    "                MEAN_ACT_CT[lesion_value][group][task] = (TOTAL_ACT_CT[lesion_value][group][task] / N_STEPs)\n",
    "                MEAN_ACT_CR[lesion_value][group][task] = (TOTAL_ACT_CR[lesion_value][group][task] / N_STEPs)\n",
    "                MEAN_ACT_LT[lesion_value][group][task] = (TOTAL_ACT_LT[lesion_value][group][task] / N_STEPs)\n",
    "                MEAN_ACT_LR[lesion_value][group][task] = (TOTAL_ACT_LR[lesion_value][group][task] / N_STEPs)\n",
    "                MEAN_ACT_ST[lesion_value][group][task] = (TOTAL_ACT_ST[lesion_value][group][task] / N_STEPs)\n",
    "                MEAN_ACT_SR[lesion_value][group][task] = (TOTAL_ACT_SR[lesion_value][group][task] / N_STEPs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06c7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITS AND PRINTING\n",
    "\n",
    "def print_heading():\n",
    "    print(\"\\n\")\n",
    "    print(\"WEAVER++/ARC model simulation of primary progressive aphasia (c) Ardi Roelofs\\n\")\n",
    "    print(\"Simulation of group studies \\n\")\n",
    "\n",
    "\n",
    "def print_parameters():\n",
    "\n",
    "    print(\"Parameter values:\\n\")\n",
    "    print(f\"cycle time : {CYCLE_TIME : 6d} [ms]\\n\")\n",
    "    print(f\"sem_rate   : {SEM_rate/STEP_SIZE : .4f} [prop/ms]\\n\")\n",
    "    print(f\"lem_rate   : {LEM_rate/STEP_SIZE : .4f} [prop/ms]\\n\")\n",
    "    print(f\"exin       : {EXTIN/STEP_SIZE : .4f} [act_units/ms]\\n\")\n",
    "    print(f\"d          : {DECAY_rate/STEP_SIZE : .4f} [prop/ms]\\n\")\n",
    "\n",
    "\n",
    "    print(\"press any key to continue \")\n",
    "\n",
    "    # getchar()\n",
    "\n",
    "    \n",
    "def compute_fits_and_print_results_on_screen(assessment):\n",
    "\n",
    "    LV = 0.0 # lesion value\n",
    "  \n",
    "    for i in range(N_GROUPs):\n",
    "        for j in range(N_TASKs):\n",
    "            SIM_DATA[i][j] = 0.0\n",
    "\n",
    "    for i in range(N_lesion_values):\n",
    "        GOODNESS_OF_FIT[i] = 0.0\n",
    "\n",
    "    if assessment == ENGLISH:\n",
    "        print(\"\\nAssessment is Savage et al. (2013), English\\n\")\n",
    "    if assessment == DUTCH:\n",
    "        print(\"\\nAssessment is Janssen et al. (2022), Dutch\\n\")\n",
    "    if assessment == BRAMBATI_T1:\n",
    "        print(\"\\nAssessment is Brambati et al. (2015), baseline T1\\n\")\n",
    "    if assessment == BRAMBATI_T2:\n",
    "        print(\"\\nAssessment is Brambati et al. (2015), follow up T2\\n\")\n",
    "    if assessment == ROHRERMANDELLI_T1:\n",
    "        print(\"\\nAssessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\\n\")\n",
    "    if assessment == ROHRERMANDELLI_T2:\n",
    "        print(\"\\nAssessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\\n\")\n",
    "\n",
    "\n",
    "    for group in range(N_GROUPs): \n",
    "        print(\" \\n\")\n",
    "\n",
    "        if group == NORMAL:\n",
    "            print(\"NORMAL \\n\")\n",
    "        elif group == NONFLUENT_AGRAMMATIC:\n",
    "            print(\"NONFLUENT/AGRAMMATIC \\n\")\n",
    "        elif group == SEMANTIC_DEMENTIA:\n",
    "            print(\"SEMANTIC DEMENTIA  \\n\")\n",
    "        elif group == LOGOPENIC:\n",
    "            print(\"LOGOPENIC  \\n\")\n",
    "\n",
    "        if assessment == ENGLISH:\n",
    "            print(\"\\nSavage et al. (2013), English\\n\")\n",
    "        if assessment == DUTCH:\n",
    "            print(\"\\nAssessment is Janssen et al. (2022), Dutch\\n\")\n",
    "        if assessment == BRAMBATI_T1:\n",
    "            print(\"\\nAssessment is Brambati et al. (2015), baseline T1\\n\")\n",
    "        if assessment == BRAMBATI_T2:\n",
    "            print(\"\\nAssessment is Brambati et al. (2015), follow up T2\\n\")\n",
    "        if assessment == ROHRERMANDELLI_T1:\n",
    "            print(\"\\nAssessment is Rohrer et al. (2013) and Mandelli et al. (2016), baseline T1\\n\")\n",
    "        if assessment == ROHRERMANDELLI_T2:\n",
    "            print(\"\\nAssessment is Rohrer et al. (2013) and Mandelli et al. (2016), follow up T2\\n\")\n",
    "\n",
    "        print(\"        Naming   Comprehension  Repetition \\n\")\n",
    "        print(f\"Real:   {REAL_DATA[group][NAMING] : 5.2f}         {REAL_DATA[group][COMPREHENSION] : 5.2f}        {REAL_DATA[group][REPETITION] : 5.2f} \\n\")\n",
    "        print(\"Lesion:                                    MAE\\n\")\n",
    "\n",
    "\n",
    "        for lesion_value in range(N_lesion_values): \n",
    "\n",
    "            print(\"/////////////\")\n",
    "            print(MEAN_ACT_ST[lesion_value][group][NAMING])\n",
    "            print(MEAN_ACT_SR[lesion_value][group][NAMING])\n",
    "            print(MEAN_ACT_ST[lesion_value][NORMAL][NAMING])\n",
    "            print(MEAN_ACT_SR[lesion_value][NORMAL][NAMING])\n",
    "            print(\"/////////////\")\n",
    "            \n",
    "            SIM_DATA[group][NAMING] = (MEAN_ACT_ST[lesion_value][group][NAMING] - MEAN_ACT_SR[lesion_value][group][NAMING]) / (MEAN_ACT_ST[lesion_value][NORMAL][NAMING] - MEAN_ACT_SR[lesion_value][NORMAL][NAMING]) * 100.0\n",
    "\n",
    "            SIM_DATA[group][COMPREHENSION] = (MEAN_ACT_CT[lesion_value][group][COMPREHENSION] - MEAN_ACT_CR[lesion_value][group][COMPREHENSION]) / (MEAN_ACT_CT[lesion_value][NORMAL][COMPREHENSION] - MEAN_ACT_CR[lesion_value][NORMAL][COMPREHENSION]) * 100.0\n",
    "\n",
    "            SIM_DATA[group][REPETITION] = (MEAN_ACT_ST[lesion_value][group][REPETITION] - MEAN_ACT_SR[lesion_value][group][REPETITION]) / (MEAN_ACT_ST[lesion_value][NORMAL][REPETITION] - MEAN_ACT_SR[lesion_value][NORMAL][REPETITION]) * 100.0\n",
    "\n",
    "\n",
    "            if group == NORMAL:\n",
    "                LV = 1.0\n",
    "            elif WEIGHT_LESION:\n",
    "                LV = WEIGHT_value[lesion_value]\n",
    "            elif DECAY_LESION:\n",
    "                LV = DECAY_value[lesion_value]\n",
    "\n",
    "            GOODNESS_OF_FIT[lesion_value] = (abs(REAL_DATA[group][NAMING] - SIM_DATA[group][NAMING])\n",
    "                + abs(REAL_DATA[group][COMPREHENSION] - SIM_DATA[group][COMPREHENSION])\n",
    "                + abs(REAL_DATA[group][REPETITION] - SIM_DATA[group][REPETITION]) ) / 3.0\n",
    "\n",
    "            if SHOW_RESULTS_ALL_VALUES: # toggle for printing the results for all lesion values \n",
    "                print(f\"{LV : 5.2f}   {SIM_DATA[group][NAMING] : 5.2f}        {SIM_DATA[group][COMPREHENSION] : 5.2f}        {SIM_DATA[group][REPETITION] : 5.2f}     {(abs( REAL_DATA[group][NAMING] - SIM_DATA[group][NAMING]) + abs( REAL_DATA[group][COMPREHENSION] - SIM_DATA[group][COMPREHENSION])+ abs( REAL_DATA[group][REPETITION] - SIM_DATA[group][REPETITION])) / 3.0 : 5.2f}\\n\")\n",
    "\n",
    "        a = 0\n",
    "        for i in range(N_lesion_values):\n",
    "            if GOODNESS_OF_FIT[a] > GOODNESS_OF_FIT[i]:\n",
    "                a = i\n",
    "\n",
    "\n",
    "        if WEIGHT_LESION:\n",
    "            print(f\"Best fit weight value = {WEIGHT_value[a] : .2f}   MAE = {GOODNESS_OF_FIT[a] : .2f}\\n\")\n",
    "        if DECAY_LESION:\n",
    "            print(f\"Best fit decay value = {DECAY_value[a] : .2f}   MAE = {GOODNESS_OF_FIT[a] : .2f}\\n\")\n",
    "\n",
    "            print(f\"Sim:   {(MEAN_ACT_ST[a][group][NAMING] - MEAN_ACT_SR[a][group][NAMING]) / (MEAN_ACT_ST[a][NORMAL][NAMING] - MEAN_ACT_SR[a][NORMAL][NAMING]) * 100.0 : 5.2f}         {(MEAN_ACT_CT[a][group][COMPREHENSION] - MEAN_ACT_CR[a][group][COMPREHENSION]) / (MEAN_ACT_CT[a][NORMAL][COMPREHENSION] - MEAN_ACT_CR[a][NORMAL][COMPREHENSION]) * 100.0 : 5.2f}        {(MEAN_ACT_ST[a][group][REPETITION] - MEAN_ACT_SR[a][group][REPETITION]) / (MEAN_ACT_ST[a][NORMAL][REPETITION] - MEAN_ACT_SR[a][NORMAL][REPETITION]) * 100.0 : 5.2f} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9245cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN ROUTINES \n",
    "\n",
    "def main():\n",
    "    ls = 0.0 # exact lesion value \n",
    "\n",
    "    print_heading()\n",
    "    print_parameters()\n",
    "    set_spreading_rates()\n",
    "\n",
    "    if WEIGHT_LESION:\n",
    "        ls = 0.0\n",
    "        for lesion_value in range(N_lesion_values):\n",
    "            # values between maximally damaged, 0.0, and minimally damaged, 0.99\n",
    "            WEIGHT_value[lesion_value] = ls \n",
    "            ls += 0.01\n",
    "\n",
    "    if DECAY_LESION:\n",
    "         # values between minimally damaged, 1.01, and maximally damaged, i.e., full decay, 1.66\n",
    "        ls = 1.01\n",
    "        for lesion_value in range(N_lesion_values):\n",
    "            DECAY_value[lesion_value] = ls\n",
    "            ls += 0.01\n",
    "\n",
    "\n",
    "    for assessment in range(N_ASSESSMENTs):\n",
    "        \n",
    "        set_real_data_matrix(assessment)\n",
    "\n",
    "        for group in range(N_GROUPs):\n",
    "\n",
    "            for task in range(N_TASKs): \n",
    "\n",
    "                for lesion_value in range(N_lesion_values): \n",
    "\n",
    "                    reset_network()\n",
    "                    set_aphasic_parameters(group, lesion_value)\n",
    "\n",
    "                    T = 0 # time in ms\n",
    "                    step = 0\n",
    "                    while T < (N_STEPs * STEP_SIZE):\n",
    "                        \n",
    "                        update_network(task, T)\n",
    "                        determine_activation_critical_nodes(lesion_value, step, group, task)\n",
    "                        \n",
    "                        T += STEP_SIZE\n",
    "                        step += 1\n",
    "\n",
    "        compute_activation_results()\n",
    "        compute_fits_and_print_results_on_screen(assessment)  \n",
    "        getchar()\n",
    "\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8c0fb",
   "metadata": {},
   "source": [
    "### Run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142c8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WEAVER++/ARC model simulation of primary progressive aphasia (c) Ardi Roelofs\n",
      "\n",
      "Simulation of group studies \n",
      "\n",
      "Parameter values:\n",
      "\n",
      "cycle time :     25 [ms]\n",
      "\n",
      "sem_rate   :  0.0101 [prop/ms]\n",
      "\n",
      "lem_rate   :  0.0074 [prop/ms]\n",
      "\n",
      "exin       :  0.1965 [act_units/ms]\n",
      "\n",
      "d          :  0.0240 [prop/ms]\n",
      "\n",
      "press any key to continue \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.array(CC_con))*SEM_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbb963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
